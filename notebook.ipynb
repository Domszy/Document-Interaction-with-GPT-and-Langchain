{"cells":[{"cell_type":"markdown","id":"bdd098e2-49c6-43df-9151-050694970b75","metadata":{},"source":["<span style=\"font-size: 25px;\">**Chat with Your Documents Using GPT & LangChain**</span>\n","\n","\n","**Objectives:** \n","- *Learn how to effectively load & store documents using LangChain*\n","- *Build a retrieval augmented generation pipeline for querying data*\n","- *Build a question-answering bot that answers questions based on your documents*\n","\n","You can learn more about the LangChain library in the following links:\n","* [How to Make Large Language Models Play Nice with Your Software Using LangChain](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)\n","* [6 Problems of LLMs That LangChain is Trying to Assess](https://www.kdnuggets.com/6-problems-of-llms-that-langchain-is-trying-to-assess)\n","\n","Let's start by understanding our main goal:\n","\n","First: \n","- Take a set of PDFs. \n","- Break them into pieces of texts. \n","- Embed them into a vectorized representation. \n","- Store them into a vector database. (FAISS, CHROMA, PINECONE...)\n","- Once the vectors are persistend in the ddbb, we can get queries, embed them and find a similar chunk vectors. \n","- The chunks are ranked according to how relevant they are to the question and are used to contextualize our LLM. \n","\n","**IMPORTANT:** The LLM doesn't really know what PDFs have. We take advantage of the LLM model to generate NLP answers and provide it with a question and a context to generate an accurate answer. \n","\n","![Structure_main](Structure_main.png)"]},{"cell_type":"markdown","id":"42239f47-628c-4f50-aa9d-ee482177dd0c","metadata":{},"source":["## 1. Install Imports and API Keys\n","\n","We need to make sure our environment has the following packages. "]},{"cell_type":"code","execution_count":20,"id":"7430d0f5-7341-464e-8d2e-c1f322edeb76","metadata":{"executionCancelledAt":null,"executionTime":53044,"lastExecutedAt":1709641405903,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install openai==0.27.1\n!pip install langchain==0.0.184\n!pip install tiktoken\n!pip install wikipedia\n!pip install pypdf\n!pip install faiss-cpu\n!pip install pinecone-client","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --quiet openai\n","%pip install --quiet langchain\n","%pip install --quiet tiktoken\n","%pip install --quiet wikipedia\n","%pip install --quiet pypdf\n","%pip install --quiet faiss-cpu\n","%pip install --quiet pinecone-client\n","%pip install --quiet pandas\n","%pip install --quiet matplotlib\n","%pip install --quiet python-dotenv\n","%pip install --quiet transformers\n","%pip install --quiet langchain_openai\n","%pip install --quiet langchain_community\n","%pip install --quiet langchain_text_splitters\n","%pip install --quiet langchain_openai\n","%pip install --quiet langchain-pinecone\n","%pip install --quiet langchain-pinecone\n","%pip install --quiet langchain_core\n","%pip install --quiet langgraph\n","%pip install --quiet langchain_anthropic"]},{"cell_type":"markdown","id":"26db4425-1415-4422-878e-fc555b2fbe0b","metadata":{},"source":["Before starting, make sure you have avaiable: \n","- OpenAI API Key\n","- Pinecone API Key and environment. \n","\n","To get our API keys, we can set them in an `.env` document and load them into our environement using the `load_dotenv()` command or define them directly. \n","- To obtain OpenAI API Keys, you can follow the instructions [here](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197). \n","- To obtain Pinecone API keys, you can follow the instructions [here](https://medium.com/forcodesake/pinecone-api-chatgpt-artificial-intelligence-4332de128dd5). "]},{"cell_type":"code","execution_count":2,"id":"0d8360cb-c7a0-4976-8fe9-5482d14b4495","metadata":{"executionCancelledAt":null,"executionTime":1591,"lastExecutedAt":1704816818752,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Basics\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\n\n# LangChain Training\n# LLM\nfrom langchain.llms import OpenAI\n\n# Document Loader\nfrom langchain.document_loaders import PyPDFLoader \n\n# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Embedding\nfrom langchain.embeddings import OpenAIEmbeddings \n\n# Vector DataBase\nfrom langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent. \n\n# Chains\n#from langchain.chains.question_answering import load_qa_chain\n#from langchain.chains import ConversationalRetrievalChain"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/domszy/Desktop/Document Interaction with GPT and Langchain/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"]}],"source":["# Basics\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from dotenv import load_dotenv\n","import langchain_core\n","\n","# LangChain Training\n","# LLM, embeddings\n","from langchain_openai import OpenAI, OpenAIEmbeddings\n","\n","# Document Loader\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# Splitter\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# Tokenizer\n","from transformers import GPT2TokenizerFast\n","\n","# Vector DataBase\n","# for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent.\n","from langchain_community.vectorstores import FAISS\n","from langchain_pinecone import PineconeVectorStore\n","\n","# Chains\n","# from langchain.chains.question_answering import load_qa_chain\n","# from langchain.chains import ConversationalRetrievalChain"]},{"cell_type":"code","execution_count":3,"id":"f0d70449-d284-43a6-ae31-df6e62ca1302","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1704816847276,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# We can directly upload our keys using a .env\n#load_dotenv()\n\nimport os\n\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\npinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\npinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n\n# Alternatively, you can set the API keys as follows:\n#OPENAI_API_KEY   = \"sk-\"\n#PINECONE_API_KEY = \"34...\"\n#PINECONE_ENV_KEY = \"gcp-starter\""},"outputs":[],"source":["# We can directly upload our keys using a .env\n","import os\n","load_dotenv()\n","\n","\n","openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n","pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n","pinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n","\n","# Alternatively, you can set the API keys as follows:\n","# OPENAI_API_KEY   = \"sk-\"\n","# PINECONE_API_KEY = \"34...\"\n","# PINECONE_ENV_KEY = \"gcp-starter\""]},{"cell_type":"markdown","id":"8a853ff5-0446-4f29-8e78-66fbe290c7f9","metadata":{},"source":["\n","# PART 1: LANGCHAIN BASICS\n","\n","\n","🎯 **Objective:** Understand what is the LangChain library and all the elements that are required to generate a simple pipeline to query out documents. \n","\n","### **What is LangChain?**\n","> LangChain is a framework for developing applications powered by language models.\n","\n","LangChain makes the hardest parts of working with AI models easier in two main ways:\n","\n","1. **Data-aware** - Bring external data, such as your files, other applications, and API data, to your LLMs\n","2. **Agentic** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next. \n","\n","### **Why LangChain?**\n","1. **Components** - Abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n","\n","2. **Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together. A structured assembly of components for accomplishing specific higher-level tasks.\n","\n","3. **Speed 🚢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n","\n","4. **Community 👥** - Wonderful discord and community support, meet ups, hackathons, etc.\n","\n","Though the usage of LLMs can be straightforward (text-in, text-out), when trying to build complex applications you'll quickly notice friction points. \n","\n","> LangChain helps with once you develop more complicated application and manage LLMs the way we want. "]},{"cell_type":"markdown","id":"1f548002-dda8-4d89-9c1e-7057392c11c5","metadata":{},"source":["## LangChain Components\n","\n","The LangChain library contains multiple elements to ease the process of building complex applications using LangChain.\n","In this module we will focus mainly in 10 elements:\n","\n","**To load and process our documents**\n","- Document Loaders\n","- Text Splitters\n","- Chat Messages *(Optional)*\n","\n","\n","**To talk with our documents using NLP**\n","- LLM model (GPT, Llama...)\n","- Chains\n","- Natural Language Retrieval\n","- Metadata and Indexes\n","- Memory *(Optional)*\n","\n","**Both Processes**\n","- Text Embedding (OpenAI or Open-source models)\n","- Vector Stores \n","\n","![Structure_basics](Structure_basics.png)\n"]},{"cell_type":"markdown","id":"46cb81ae-fe1d-4108-a050-2e66c1bbf296","metadata":{},"source":["###  **The Model - Large Language Model of our choice**\n","An AI-powered LLM that takes text in and responses text out. \n","The default model is always ada-001, but we can explicitly choose the model of our preference. \n","\n","You can check the list of all avaialble models [here](https://platform.openai.com/docs/models)"]},{"cell_type":"code","execution_count":9,"id":"f1c96d08-9a35-40f5-a248-f0cb74a842fc","metadata":{"executionCancelledAt":null,"executionTime":4360,"lastExecutedAt":1704817021400,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.llms import OpenAI\n\nchatgpt = OpenAI(\n                 model_name = \"gpt-3.5-turbo\", \n                 temperature= 0\n)\n\nprompt=\"Please, tell me some funny jokes\"\n\nprint(chatgpt(prompt))","outputsMetadata":{"0":{"height":500,"type":"stream"},"1":{"height":437,"type":"stream"}}},"outputs":[{"data":{"text/plain":["\"\\n\\n1. Why don't scientists trust atoms? Because they make up everything.\\n\\n2. What do you call a fake noodle? An impasta.\\n\\n3. Why couldn't the bicycle stand up by itself? Because it was two-tired.\\n\\n4. What do you call an alligator in a vest? An investi-gator.\\n\\n5. Why did the tomato turn red? Because it saw the salad dressing.\\n\\n6. What do you call a belt made out of watches? A waist of time.\\n\\n7. How do you organize a space party? You planet.\\n\\n8. Why did the scarecrow win an award? Because he was outstanding in his field.\\n\\n9. What do you call a fish wearing a bowtie? Sofishticated.\\n\\n10. Why did the math book look sad? Because it had too many problems.\\n\\n11. What do you call a bear with no teeth? A gummy bear.\\n\\n12. Why did the chicken go to the seance? To get to the other side.\\n\\n13. What do you call a sleeping dinosaur? A dino-snore.\\n\\n14. Why did the tomato turn red? Because it saw the salad dressing.\\n\\n15. What do you call a belt made out of watches? A waist of time.\\n\\n\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","\n","model = OpenAI(\n","    model=\"gpt-3.5-turbo-instruct\",\n","    temperature=0\n",")\n","\n","# using prompt templates in this case\n","prompt_template = PromptTemplate.from_template(\n","    template=\"Please, tell me some funny jokes\"\n",")\n","\n","completion_chain = prompt_template | model\n","\n","completion_chain.invoke({})"]},{"cell_type":"markdown","id":"e039302b-02bb-499c-8637-f4be1bd2e34c","metadata":{},"source":["### **Chat Messages**\n","LangChain allows us to segmentate prompts into three main types.(System, Human, AI)\n","\n","* **System** - Helpful background context that tell the AI its high-level behavior.\n","* **Human** - Messages that represent the user input. \n","* **AI** - Messages that show the response of the AI model, they work as examples to the model. \n","\n","\n","For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"]},{"cell_type":"code","execution_count":12,"id":"5a6c86bf-542a-43f3-a1e7-221ed9bf6918","metadata":{"executionCancelledAt":null,"executionTime":954,"lastExecutedAt":1704817116502,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nhigh_level_behavior = \"\"\"\n                       You are an AI bot that help people decide where to travel. \n                       Always recommend three destination with a short sentence for each.\n                      \"\"\"\n\nresponse = chatgpt(\n    [\n        SystemMessage(content=high_level_behavior),\n        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n        HumanMessage(content=\"Where should I travel next?\"),\n    ]\n)\n\nprint(response.content)","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":57,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Here are three destination recommendations for you:\n","\n","1. Bali, Indonesia - Explore stunning beaches, lush rice terraces, and vibrant cultural attractions in this tropical paradise.\n","2. Barcelona, Spain - Immerse yourself in the vibrant art, architecture, and culinary scene of this dynamic city by the Mediterranean Sea.\n","3. Banff National Park, Canada - Experience the breathtaking beauty of the Canadian Rockies with crystal-clear lakes, towering mountains, and abundant wildlife.\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","\n","model = ChatOpenAI(\n","    temperature=0,\n","    max_tokens=1000\n",")\n","\n","high_level_behavior = \"\"\"You are an AI bot that help people decide where to travel. Always recommend three destination with a short sentence for each.\"\"\"\n","\n","response = model.invoke(\n","    [\n","        SystemMessage(content=high_level_behavior),\n","        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n","        HumanMessage(content=\"Where should I travel next?\"),\n","    ]\n",")\n","\n","print(response.content)"]},{"cell_type":"markdown","id":"ab221be1-08fd-4942-b06e-5945cce7b148","metadata":{},"source":["You can also pass more chat history with responses from the AI"]},{"cell_type":"code","execution_count":null,"id":"035d9b68-ff7c-48fe-98e4-76961df7c480","metadata":{"executionCancelledAt":null,"executionTime":3077,"lastExecutedAt":1704817121541,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"response = chatgpt(\n        [\n            SystemMessage(content=high_level_behavior),\n            AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n            HumanMessage(content=\"Where should I travel next?\"),\n            SystemMessage(content=\"What do you enjoy doing?\"),\n            HumanMessage(content=\"I love going to Museums?\"),\n        ]\n    )\n\nprint(response.content)","outputsMetadata":{"0":{"height":227,"type":"stream"},"1":{"height":217,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Based on your love for museums, here are three destinations you might enjoy:\n","\n","1. Paris, France: Known as the \"City of Museums,\" Paris is home to world-renowned museums like the Louvre, Musée d'Orsay, and Centre Pompidou, offering a rich collection of art, history, and culture.\n","\n","2. Florence, Italy: Florence is a treasure trove of art and history, with museums like the Uffizi Gallery and Accademia Gallery housing masterpieces by Michelangelo, Botticelli, and more, making it a haven for art enthusiasts.\n","\n","3. St. Petersburg, Russia: St. Petersburg boasts the Hermitage Museum, one of the largest and most prestigious art museums in the world, showcasing a vast collection of art and cultural artifacts from around the globe.\n"]}],"source":["response = model(\n","    [\n","        SystemMessage(content=high_level_behavior),\n","        AIMessage(\n","            content=\"Hello! I am a traveller assistant, how can I help you?\"),\n","        HumanMessage(content=\"Where should I travel next?\"),\n","        SystemMessage(content=\"What do you enjoy doing?\"),\n","        HumanMessage(content=\"I love going to Museums?\"),\n","    ]\n",")\n","\n","print(response.content)"]},{"cell_type":"markdown","id":"cf8f2cdd-0466-423f-8984-45a5e54585af","metadata":{},"source":["### **Text Embedding Model**\n","\n","When documents or string-variables are too long, things can get quite complicated. \n","\n","**In order to be able to process them, we can embed and convert string variables into vectors** (a series of numbers that hold the semantic 'meaning' of your text).\n","\n","Mainly used when comparing different pieces of text or when dealing with huge texts. "]},{"cell_type":"markdown","id":"fd391e69-5df4-4de4-8625-c590fb372227","metadata":{},"source":["**TASK:**\n","- First import the `Embeddings` model from langcgain.embeddings.\n","- Define a text to embed. \n","- Embed the text with the `.embed_query` command. "]},{"cell_type":"code","execution_count":13,"id":"3980f040-5612-4b0b-9a2d-3b9e3c8b2ba4","metadata":{"executionCancelledAt":null,"executionTime":157,"lastExecutedAt":1704817207570,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the embedding model\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 2. Create an instance of the model\nembeddings = OpenAIEmbeddings()\n\n# 3. Define a text to embed\ntext = \"This is a webinar!\"\n\n# 4. Embed the text\ntext_embedding = embeddings.embed_query(text)\n\nprint (f\"Your embedding is length {len(text_embedding)}\")\nprint (f\"Here's a sample: {text_embedding[:5]}...\")","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Your embedding is length 1536\n","Here's a sample: [-0.0022637732326984406, -0.003931235522031784, -0.0011279426980763674, -0.01740344613790512, -0.021252648904919624]...\n"]}],"source":["# 1. Create an instance of the model\n","embeddings = OpenAIEmbeddings()\n","\n","# 2. Define a text to embed\n","text = \"Hi! It's time to go to a Museum!\"\n","\n","# 3. Embed the text\n","text_embedding = embeddings.embed_query(text)\n","print(f\"Your embedding is length {len(text_embedding)}\")\n","print(f\"Here's a sample: {text_embedding[:5]}...\")"]},{"cell_type":"markdown","id":"35ae7808-4897-4435-aa62-b29625f0c5aa","metadata":{},"source":["### Memory\n","When interacting with a model, it is important to keep track of all interactions performed with it. LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n","\n","It is important to consider that storing all the interactions with the model can quickly escalate to a considerable amount of tokens to process every time we prompt the model. It is essential to bear in mind that ChatGPT has a token limit per interaction.\n","\n","You can learn more about memory [here]([https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/])"]},{"cell_type":"code","execution_count":10,"id":"e6dd11b7-b05e-435b-b8f2-27505a395c75","metadata":{"executionCancelledAt":null,"executionTime":4966,"lastExecutedAt":1704817328092,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.memory import ConversationSummaryBufferMemory\n\nmemory = ConversationSummaryBufferMemory(llm=chatgpt, max_token_limit=100)\n\nmemory.save_context({\"input\":  \"Can you recommend me where should I travel next?\"}, \n                    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"})\n\nmemory.save_context({\"input\":  \"I love going to Museums\"}, \n                    {\"output\": \"Great then you should go to a cultural capital.\"})\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nconversation = ConversationChain(\n    llm=chatgpt, \n    memory = memory,\n    verbose=True\n)\n\nconversation.run(\"What cities do you recommend me?\")","outputsMetadata":{"0":{"height":353,"type":"stream"},"1":{"height":337,"type":"stream"},"2":{"height":97,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","I love going to Museums\n","================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," and learning about history and different cultures. It's fascinating to see how people lived in the past and how it has shaped our world today. Plus, museums often have interactive exhibits and activities that make learning even more fun. It's a great way to spend a day and expand my knowledge.\n","================================\u001b[1m Human Message \u001b[0m=================================\n","\n","Can you recommend me where should I travel next?\n","================================\u001b[1m Human Message \u001b[0m=================================\n","\n","\n","AI: That depends on your interests and budget. Some popular destinations for travelers include Paris, Rome, Tokyo, and New York City. If you enjoy nature and outdoor activities, you may want to consider visiting national parks or going on a safari in Africa. If you're interested in history and culture, places like Athens, Istanbul, and Cairo could be great options. Ultimately, it's important to research and plan a trip that aligns with your interests and budget.\n"]}],"source":["import uuid\n","from IPython.display import Image, display\n","from langchain_core.messages import HumanMessage\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","# Define a new graph\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","# Define the function that calls the model\n","\n","\n","def call_model(state: MessagesState):\n","    response = model.invoke(state[\"messages\"])\n","    return {\"messages\": response}\n","\n","\n","# Define the two nodes we will cycle between\n","workflow.add_edge(START, \"model\")\n","workflow.add_node(\"model\", call_model)\n","\n","# Adding memory is straight forward in langgraph!\n","memory = MemorySaver()\n","\n","app = workflow.compile(\n","    checkpointer=memory\n",")\n","\n","# The thread id is a unique key that identifies\n","# this particular conversation.\n","# We'll just generate a random uuid here.\n","# This enables a single application to manage conversations among multiple users.\n","thread_id = uuid.uuid4()\n","config = {\"configurable\": {\"thread_id\": thread_id}}\n","\n","input_message = HumanMessage(content=\"I love going to Museums\")\n","for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n","    event[\"messages\"][-1].pretty_print()\n","\n","# Here, let's confirm that the AI remembers our previous messsage!\n","input_message = HumanMessage(\n","    content=\"Can you recommend me where should I travel next?\")\n","for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n","    event[\"messages\"][-1].pretty_print()"]},{"cell_type":"markdown","id":"e48c27ca-2d99-45d1-9d64-3436d095765e","metadata":{},"source":["### Dealing with Documents\n","\n","We are here to deal with documents... so LangChain provides a wide variety of elements to deal with them. \n","\n","One of the most important improvements of LangChain is that it allows us to upload documents and pass them to our model. \n","We consider a document as an object that holds a piece of text and metadata (more information about that text)\n","\n","- Document class\n","- Document Loader\n","- Document Retriever\n","- Text Splitter\n","- Index"]},{"cell_type":"markdown","id":"bd360d81-3068-44b3-b72b-451623776022","metadata":{},"source":["**TASK**\n","\n","1. From langchain.schema import the `Document` class. \n","2. Now define a document that has \n","   - Text contained in page_content. \n","   - Metada composed of document_id, document_source and document_create_time. "]},{"cell_type":"code","execution_count":8,"id":"52d33238-ce16-45fd-b216-1b9166341193","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1704817584415,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the document class\nfrom langchain.schema import Document\n\n# 2. Define the document:\nDocument(\n         page_content=\"This is a dummy document\",\n         metadata={\n             'document_id' : 677,\n             'document_source' : \"mysource.pdf\",\n             'document_create_time' : \"01/06/2022\"\n                   })"},"outputs":[{"data":{"text/plain":["Document(metadata={'document_id': 0, 'document_source': 'my_source', 'document_create_time': '01/01/2000'}, page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Import the document class\n","from langchain_core.documents import Document\n","\n","# 2. Define the document:\n","Document(page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\",\n","         metadata={\n","             'document_id': 0000,\n","             'document_source': \"my_source\",\n","             'document_create_time': \"01/01/2000\"\n","         })"]},{"cell_type":"markdown","id":"8c33e2ea-572e-4fbd-a348-5bcf0c961a8b","metadata":{},"source":["#### Document Loaders\n","\n","Depending on where our data is stored, we will need a different type of loader:\n","\n","- The **Online Loader** is used for loading a document directly from the Internet. LangChain implements different types of loaders. For example, there is the `WikipediaLoader` that helps you loading Wikipedia pages or the `HNLoader` to take content directly from any HackerNews page.\n","\n","\n","\n","- The **Offline Loader** is used loading a document stored that are already installed in your machine. There are also different types of offline loaders such as the **HTML** loader for `.html` pages or the **PyPDFLoader** for `.pdf` documents.\n","\n","In this project, we will see an example of Online Loader by using the `WikipediaLoader` and the `HNLoader`, and an example of Offline Loader by using the PyPDFLoader.\n","\n","You can find a list of the supported [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders) in the official documentation. Those Loaders are from external integrations, [native LangChain Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be found in the official documentation as well."]},{"cell_type":"code","execution_count":12,"id":"2599ff89-9838-4341-91a9-424eb3c4a49d","metadata":{"executionCancelledAt":null,"executionTime":5701,"lastExecutedAt":1704817696213,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.document_loaders import WikipediaLoader\n \n# Load content from Wikipedia using WikipediaLoader\nloader = WikipediaLoader(\"Machine_learning\")\nwikipedia_data = loader.load() #It returns a list of documents\n\nwikipedia_data[0]"},"outputs":[{"data":{"text/plain":["'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many field'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_community.retrievers import WikipediaRetriever\n","\n","# Load content from Wikipedia using WikipediaLoader\n","retriever = WikipediaRetriever()\n","# It returns a list of documents\n","wikipedia_data = retriever.invoke(\"Machine Learning\")\n","\n","wikipedia_data[0].page_content[:400]"]},{"cell_type":"code","execution_count":13,"id":"d3dca600-e561-4654-b688-7cec585ae80d","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1704817744183,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"\\nPage Content: \\n\", wikipedia_data[0].page_content)\nprint(\"\\nMeta Data: \\n\", wikipedia_data[0].metadata)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Page Content:  Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\n","ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\n","Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \n","From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n","\n","\n","== History ==\n","\n","The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\n","Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\n","By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\n","Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\n","Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetic\n","\n","Meta Data:  {'title': 'Machine learning', 'summary': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.', 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}\n"]}],"source":["print(\"\\nPage Content: \", wikipedia_data[0].page_content)\n","print(\"\\nMeta Data: \", wikipedia_data[0].metadata)"]},{"cell_type":"markdown","id":"a6c5bc9d-5c18-4bb6-b30b-025da04e0862","metadata":{},"source":["**TASK:**\n","\n","Repeat the previous procedure using `HNLoader` and `PyPDFLoader`. \n","1. Import the corresponding Loader from langchain.document_loaders. \n","2. Initialize the loader indicating the source of data. \n","    - HNLoader -> https://news.ycombinator.com/item?id=34422627\n","    - PDFLoader -> Docs/attentions.pdf\n","    - PDFDirectoryLoader -> Docs/"]},{"cell_type":"code","execution_count":15,"id":"3a7b588e-8146-4df2-8e85-2e7d6d89cbb0","metadata":{"executionCancelledAt":null,"executionTime":2520,"lastExecutedAt":1704817928107,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Online Loader\nfrom langchain.document_loaders import HNLoader\nloader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\nhn_data = loader.load()\n\n# Load content from local PDFs\nfrom langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_data = loader.load()\n\n#We can use a directory loader to load more than one PDF at once. \n#loader = PyPDFDirectoryLoader(\"Docs/\")\n#pdf_directory_data = loader.load()"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Ozzie_osman on Jan 18, 2023  \\n             | next [–] \\n\\nLangChain is awesome. For people not sure what it\\'s doing, large language models (LLMs) are very powerful but they\\'re very general. As a common example for this limitation, imagine you want your LLM to answer questions over a large corpus.You can\\'t pass the entire corpus into the prompt. So you might:\\n- preprocess the corpus by iterating over documents, splitting them into chunks, and summarizing them\\n- embed those chunks/summaries in some vector space\\n- when you get a question, search your vector space for similar chunks\\n- pass those chunks to the LLM in the prompt, along with your questionThis ends up being a very common pattern, where you need to do some preprocessing of some information, some real-time collecting of pieces, and then an interaction with the LLM (in some cases, you might go back and forth with the LLM). For instance, code and semantic search follows a similar pattern (preprocess -> embed -> nearest-neighbors at query time -> LLM).Langchain provides a great abstraction for composing these pieces. IMO, this sort of \"prompt plumbing\" is far more important than all the slick (but somewhat gimicky) \"prompt engineering\" examples we see.I suspect this will get more important as the LLMs become more powerful and more integrated, requiring more data to be provided at prompt time.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Ozzie_osman on Jan 18, 2023  \\n             | parent | next [–] \\n\\nAlso, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index) which takes a more \"data structure\" approach (and actually uses langchain for some stuff under the hood).'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"fireant on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nThank you for the link. I have a project with plenty of structured data from which I create summaries and short articles. Currently I'm generating prompts with context inside the prompt. GPT Index seems to pretty well fitted to that usecase\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bluecoconut on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis is great! I love seeing how rapidly in the past 6 months these ideas are evolving. I\\'ve been internally calling these systems \"prompt machines\". I\\'m a strong believer that chaining together language model prompts is core to extracting real, and reproducible value from language models. I sometimes even wonder if systems like this are the path to AGI as well, and spent a full month \\'stuck\\' on that hypothesis in October.Specific to prompt-chaining: I\\'ve spent a lot of time ideating about where \"prompts live\" (are they best as API endpoint, as cli programs, as machines with internal state, treated as a single \\'assembly instruction\\' -- where do \"prompts\" live naturally) and eventually decided on them being the most synonymous with functions (and api endpoints via the RPC concept)mental model I\\'ve developed (sharing in case it resonates with anyone else)a \"chain\" is `a = \\'text\\'; b = p1(a); c = p2(b)` where p1 and p2 are LLM prompts.What comes next (in my opinion) is other programming constructs: loops, conditionals, variables (memory), etc. (I think LangChain represents some of these concepts as their \"areas\" -> chain (function chaining), agents (loops), memory (variables))To offer this code-style interface on top of LLMs, I made something similar to LangChain, but scoped what i made to only focus on the bare functional interface and the concept of a \"prompt function\", and leave the power of the \"execution flow\" up to the language interpreter itself (in this case python) so the user can make anything with it.https://github.com/approximatelabs/lambdapromptI\\'ve had so much fun recently just playing with prompt chaining in general, it feels like the \"new toy\" in the AI space (orders of magnitude more fun than dall-e or chat-gpt for me). (I built sketch (posted the other day on HN) based on lambdaprompt)My favorites have been things to test the inherent behaviors of language models using iterated prompts. I spent some time looking for \"fractal\" like behavior inside the functions, hoping that if I got the right starting point, an iterated function would avoid fixed points --> this has eluded me so far, so if anyone finds non-fixed points in LLMs, please let me know!I\\'m a believer that the \"next revolution\" in machine-written code and behavior from LLMs will come when someone can tame LLM prompting to self-write prompt chains themselves (whether that is on lambdaprompt, langchain, or something else!)All in all, I\\'m super hyped about LangChain, love the space they are in and the rapid attention they are getting~'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"gandalfgeek on Jan 18, 2023  \\n             | parent | next [–] \\n\\nLambdaPrompt looks really cool. Love the Pythonic expression as good ol' functions, which makes the code read very naturally.What applications were you envisioning when you built it?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bluecoconut on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIn terms of applications with it, I have made things like sketch: https://github.com/approximatelabs/sketchRaw prompt-structure ideas i\\'ve worked with:- Iterate on a prompt with another \"discriminator\" prompt, that determines the result is good / safe- Write N-trials of an answer, then use another prompt to select the best answer- When doing code writing (SQL or Pandas) write the output, then use a parser (eg. `ast` in python) to validate code is valid, if not, feed back into a prompt for fixes- Logical negation checks (check if X, and if ~X, give opposite answers, then it\\'s likely consistent, if it\\'s both \"affirmative\" (as the models tend to bias towards), then it\\'s definitely hallucinating)Other \\'product\\' ideas i\\'ve tried:- A chat style interface (I made a chat-bot last year, similar to chatGPT)- A \"google-this-for-me\" style chain, that checks google, summarizes multiple results, then synthesizes a final resultIdeas I\\'ve been sitting on, that I think would be fun to prototype:- An iterative \"large document\" editor: storing global intent, instructions, outline, and the raw text, and each iteration of the prompt works on the these objects to build a large document.- A \"research this topic for me\", similar to the above, but include the google searching, summarizing, and such- A code-repository \"AI agent\" that takes `Issues` and `Pull requests` as input, and writes and edits code for you, and by adding feedback in github, it uses that to modify the branch and act as a developer. (Code via github interface, rather than an IDE)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='clxy on Jan 19, 2023  \\n             | root | parent | next [–] \\n\\nThanks for sharing your ideas! With respect to the research this topic for me concept, I stumbled upon this repo:https://github.com/daveshap/LiteratureReviewBotIt does something similar, but uses the ArXiv dataset to search PDFs instead of the internet.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"trentearl on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nThis resonates with me. I've been slowly working on similar project, instead of python functions I'm doing api creation. I have a website that allows you to create templated apis for each prompt.The next step for me is the workflow composition part. Instead of a functional  model of python functions Im going to try to compose workflows with AWS step functions where each step function calls a particular one of the templated api endpoints.I'm excited to see your progress.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | prev | next [–] \\n\\nLangChain is very cool. Templates and composability are the way forward.I'm guessing that most folks haven't created composable prompts, so a few experiences on why this stuff is necessary. I've been building a language learning app and it makes heavy use of GPT-3 [1], which has made clear a number of things for me:1) Composability is fundamental to leveraging language models. You can't get language models to just generate things in one go. It's rather like a human. If you're writing a book, you start with an idea, then an outline, then a chapter outline, then writing paragraphs... Or in our case, generate a flashcard deck description, then vocab, then example sentences, etc.2) Externalized prompt templates are also important. Engineers need to be able interface with experts who can create custom prompts. E.g. in our case, I need experts who speaks to build prompts specific to other languages for a language learning app [2].3) Unit testing is critical. There is no linter for a prompt. I have made so many typos over the last year that broke things. OpenAI has released several new models over the last 2 years. Anthropic is coming out with a model. You need to have assurances your prompts work. I've actually had to start building a basic unit tester for our prompts because of this... [3] (please someone else do this so I don't have to)4) external data is the next step forward for large language models. E.g. in my case, someone may want to learn about the history/culture of a country and we may want to reference existing articles on it since LLMs are known to hallucinate. We need to be able to interface with the web and databases easily. I'm not convinced that LangChain is the write layer of abstraction for this. I suspect/hope that the next version of GPT will have some significant advances in this regard.1. https://squidgies.app2. https://github.com/squidgyai/squidgy-prompts - open source composable prompts for Squidgies3. https://github.com/squidgyai/squidgy-prompts/tree/main/tests - unit test in YAML\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='beepbooptheory on Jan 18, 2023  \\n             | parent | next [–] \\n\\nSo much effort and processing power going into narrowing down and coercing a vastly general thing, makes me wonder at what point it stops being worth it to use the LLM.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='crosen99 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIt’s a lot of work and it’s messy, but the multiplier on productivity is too large to ignore. At some point all this will get easier, but for now it’s do or die.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='beepbooptheory on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nAfter the dust settles, what is it really multiplying?  Especially accounting for the energy/gpus it takes.  At the very least, why not generate all the cards once, and edit as needed?I just feel like this entire concept verges on this: https://imgur.com/EiGL1Z0'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='tintor on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nUnit testing: How would you automatically verify LLM output to a static prompt? Outputs can vary with each invocation.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='dandiep on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nTemperature = 0 solves a lot of it, but not all. A list of all options solves a portion of the remaining. Not sure what to do about the rest - maybe ask the LLM?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='tennisfan118 on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nBuilding https://promptable.ai to solve testing!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nAwesome... but I can't tell from your website what exactly it is.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='xrd on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nFWIW, your signup form has a field that creeps beyond the borders.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='dang on Jan 18, 2023  \\n             | prev | next [–] \\n\\nRelated ongoing thread:GPT-3.5 and Wolfram Alpha via LangChain - https://news.ycombinator.com/item?id=34422122'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"cs702 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nVery cool.By far the most interesting aspect of this, for me, is that we're now seeing tools for building software infrastructure with layers of APIs that operate on -- gasp! -- natural language, which is notoriously prone to imprecision and ambiguity. And yet it works remarkably well. It's hard not to look at all this, mouth agape, in awe.Part of me wonders, though:Wouldn't it be better if we could compose LLMs by passing sequences of embeddings (e.g., in a standardized high-dimensional space), which are much richer representations of LLM input, internal, and output states?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='firasd on Jan 18, 2023  \\n             | parent | next [–] \\n\\nI think embeddings are actually low-\\'resolution\\' representations. Like GPT\\'s ability to parse and calculate the structure of the sentence \"Hello, how are you?\" is not represented in the embedding for the sentence. The embedding is a 1-dimensional vector and inside the model it interacts with other texts in 10,000+ dimensions'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='neuronexmachina on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nI wonder if there\\'d be any use for a \"ontological\" representation, somewhere in-between a natural-language string and its embedding in a particular LLM. Maybe something that balances human-readability, LLM-composability, lack of brittleness, insight into the local structure of the embedding, etc.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nI wonder too. I imagine the best we could do with present technology is to get back the generated text in the form of text tokens accompanied by their corresponding deep embeddings (last hidden states): `[(text_token, deep_emb), (text_token, deep_emb), ...]`. Those deep embeddings incorporate \"everything the model knows\" about each generated token of text.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='neuronexmachina on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nMaybe a mapping/representation for a \"medium embedding\" could be learned that strikes a balance between shallow and deep. I have no idea what a good objective-function would be, though.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nI mean deep embeddings (i.e., sequences of hidden states, the ones are computed by all those interactions) , not the shallow embeddings of token ids in the first layer of the model! Those deep embeddings are much richer representations.Imagine if you and others building apps had access to \"GPT3 deep sequence embeddings v1.0\" via an API.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='machiaweliczny on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIt’t that precisely embeddings API from OpenAI? It has all context so very useful for search'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nNot quite. My understanding is that OpenAI\\'s various embeddings APIs return only a single vector per document, instead of the sequence of hidden states corresponding to each predicted next token in the response generated by a GPT-type LLM.Imagine getting generated text from a GPT LLM that comes with a deep embedding of each generated token\\'s \"contextual meaning\":  [(text_token, deep_emb), (text_token, deep_emb), ...]\\n\\nallowing higher-level models and apps to use all the information in those rich representations as inputs.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='swyx on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\n> And yet it works remarkably well.by which measure are you making this claim? even a 95% reliability means you get 5% wrong. on top of that you have prompt injection attacks. this stuff is much less suitable the more you move away from demos to predictable business applications'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nWhoa, I didn\\'t say this is suitable for predictable business applications yet!What I did say is that I\\'m in awe at the fact that this stuff works as well as it does, given that natural language is so notoriously prone to imprecision and ambiguity. I mean, if you had told me six months ago that this would be working even \"95%\" of the time in demos, I would have said, no way.Basically, I agree with you that at present this becomes \"less suitable the more you move away from demos to predictable business applications\" :-)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nPerhaps, but language is the common denominator in a multi-model world. E.g., I pass the GPT output into other models which are fine tuned for that sub domain. You can do embedding to embedding conversion, but not sure it's worth the effort.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nImagine if OpenAI made GPT3\\'s final hidden states available via an API (\"GPT3 deep sequence embeddings v1.0\"), next to each generated text token: [(text_token, deep_emb), (text_token, deep_emb), ...]. You and anyone else could build apps on top. Those hidden states would incorporate much more, and much richer, information than the text. Higher-level models could be trained to act on such information!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='LASR on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis is highly useful.I am prototyping new features where I work, on top of GPT3. To get it beyond fancy demos and actually delivering customer utility, you need a LOT of work to build in robustness and correctness.Prompt chains is where I’ve landed at the moment. This library seems very relevant and timely.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"alooPotato on Jan 18, 2023  \\n             | parent | next [–] \\n\\nIs your usage of GPT3 user facing in realtime? When you implement prompt chains, doesn't the UX become really slow?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"LASR on Jan 20, 2023  \\n             | root | parent | next [–] \\n\\nIt works well if you continously provide progress to the user. In my specific use case, the data generated is progressively displayed to the user, and refined in many further chain steps. They see the evolution of this. So it feels like it's fast, but also that a bunch of work is happening.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='dandiep on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nYes.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='jsemrau on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\n>you need a LOT of work to build in robustness and correctness.While bias is the more politically charged problem, variance is the problem of the current iteration that needs to be solved.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"hooande on Jan 18, 2023  \\n             | prev | next [–] \\n\\nIsn't this a chat interface to various apis? I guess that's the point. But it doesn't seem like that is utilizing the power of large language models. Maybe I'm not understanding this.Using a Google search api + a calculator to answer a question is cool [0]. but... we could already do that?[0] https://langchain.readthedocs.io/en/latest/modules/agents/ex...\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bob29 on Jan 18, 2023  \\n             | parent | next [–] \\n\\n>but... we could already do that?\\nthat sums up so much.\\nthe emperor has no clothes. few see it. \\nits inferior to the modern purpose built tools.\\nwe act impressed a simple program can be created from a prompt, what has every programmer been doing already for the last 10 years with google search and stackoverflow?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='motoxpro on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nAnother comment from this thread seems pretty cool to me.\\nhttps://github.com/approximatelabs/sketch'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='yunwal on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nI think building regexes is a good example of where chatgpt/copilot is useful. It’s not that it’s particularly difficult or requires much understanding, just very time consuming compared to writing an English language description and walking through an example.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bob29 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nUseful compared to writing a regex in notepad yeah.Last time I was challenged by regex I easily found very fancy web page with so many nice features. Actual documentation, ability to select a specific regex engine (or implementation or whatever you call it) , real-time results on test data, highlighting that shows how the regex works, etc. \\nand that was years ago I’m sure there’s even better web apps now.I can’t imagine having a better experience asking AI chat than using a web app made for the purpose'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='yunwal on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nDo you have a link to the site?Being able give some examples and just state in plain English what you want the capture groups to be is pretty much my ideal regex experience (in other words, I don’t want to think about the semantics of regex ever).'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"sandkoan on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nThis might be what they're referring to: https://regex101.com/.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='yunwal on Jan 19, 2023  \\n             | root | parent | next [–] \\n\\nThat’s what I figured. Maybe it’s just me but for complicated use-cases this still takes me forever to get the right regex string, especially with capture groups.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='KennyFromIT on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nThey have a dedicated chat bot for their docs that might be able to help you find answers to your questions...https://chat.langchain.dev/'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='user- on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nMe\\n> can i use docker?AI\\n> Yes, you can use Docker with LangChain. For more information, please see the Docker Installation Guide in the LangChain documentation.Then it links to a 404 lol. I checked the docs and there is nothing about docker in them. I wonder why its incorrect here'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"andreigheorghe on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nmaybe it's trained on non-published documentation pages?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"sebastiennight on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nI'm very curious as to how they are able to limit the chat bot's knowledge to their own documentation.It seems as if it should also have the entire knowledge of the LLM, but> Who was Thomas Jefferson?Outputs> Hmm, I'm not sure. I'm an AI assistant for the open source library LangChain. You can find more information about LangChain at https://langchain.readthedocs.io.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='rafaquintanilha on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nYou can actually train the model to have access only to a specific body of text, like a documentation.Very useful article explaining this approach: https://dagster.io/blog/chatgpt-langchain'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='sebastiennight on Jan 20, 2023  \\n             | root | parent | next [–] \\n\\nWow that was a fascinating read! Thank you.It doesn\\'t explain why the model only refers to the documentation though.\\nBasically what they are doing is giving GPT-3 a prompt that includes the (semantically relevant) pieces of the documentation.But I don\\'t see why a User can\\'t ask about something that is tangentially relevant (\"Who is Bill Gates?\") and get an answer that really comes from GPT-3 pre-existing knowledge.And very clever way for these guys to go from \"hey we found this cool problem\" to \"well, did you notice that it ends up super complex and slow? Well well well, we could make it so much better with... wait for it... a data pipeline!\"(\"...and don\\'t we just happen to sell data pipeline software! What a coincidence!\")lol.\\nGreat read, thank you for the recommendation.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"mahastore on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nYes also the part I don't understand is that how langchain can be used to make the agents smarter for a particular domain? I did not see any interface to provide feedback of feed data. Maybe I am missing something?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Ozzie_osman on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIt\\'s usually either providing data to the LLM, or doing back-and-forth with the LLM (usually a mix of both).For instance, if you want it to answer questions about your code-base, the model doesn\\'t know your code base. You can\\'t feed the entire code-base into a prompt. So, you\\'d use langchain to:\\n  - preprocess your code-base, by chunking it and embedding it in some vector space\\n  - when you get a question, see where it is in the vector space and find the \"k nearest neighbors\"\\n  - pass those nearest neighbors, along with your question, to the LLM (because those neighbors are the contextually relevant pieces, and they\\'d fit in the prompt)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='delijati on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\ni would really love to give it a try but with an offline LLM :/'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='peterth3 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nWhat LLMs does LangChain support?Btw I asked chat.langchain.dev and it said:> LangChain uses pre-trained models from Hugging Face, such as BERT, GPT-2, and XLNet. For more information, please see the Getting Started Documentation[0].That links to a 404, but I did find the correct link[1]. Oddly that doc only mentions an OpenAI API wrapper. I couldn’t find anything about the other models from huggingface.Does LangChain have any tooling around fine tuning pre-trained LLMs like GPTNeoX[2]?[0]https://langchain.readthedocs.io/en/latest/getting_started.h...[1]https://langchain.readthedocs.io/en/latest/getting_started/g...[2]https://github.com/EleutherAI/gpt-neox'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='lgas on Jan 18, 2023  \\n             | parent | next [–] \\n\\nHere are the docs on the built in models at the moment: https://langchain.readthedocs.io/en/latest/reference/modules...One of their examples is    from langchain import NLPCloud\\n    nlpcloud = NLPCloud(model=\"gpt-neox-20b\")\\n\\nSo it looks like you\\'re good to go.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='throwaway20222 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis is amazing. Thank you to all the contributors!I am working on a project in the old media space and we were planning to build several of these elements ourselves if something like this didn’t come along. Love the open source nature and would love to contribute.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='brotchie on Jan 18, 2023  \\n             | prev | next [–] \\n\\nHell yeah. Been thinking about this in my spare cycles a lot! The missing thing when interacting with GPT3 was building up layers of re-usable abstraction that could be mixed together: i.e. a prompt that\\'s great at summarization, a prompt that great at generating a list of 5 x related ideas, a prompt that\\'s great at sorting a list of inputs according to some verbal sorting description.Had a play around with this early-GPT3 days by creating a Python decorator that let you easily write \"prompt functions\" and then you could combine these to create higher-level prompt generating machines.LangChain takes this to the logical end-point, awesome!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='owlninja on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI like to think I am smart and in the loop - but would this allow me to feed all sorts of PowerPoint presentations, word docs, text files, etc... so that new hires could get a concise answer to business processes?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | parent | next [–] \\n\\nThere's a lot of work going on in this area right now. Check out GPT Index:https://github.com/jerryjliu/gpt_index\\nhttps://github.com/jerryjliu/gpt_index/blob/main/examples/pa...GPT isn't multi-modal yet (so no images), but that's coming.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='camjw on Jan 18, 2023  \\n             | prev | next [–] \\n\\nWe’re using LangChain at my startup to orchestrate all the GPT-3 calls and it’s fantastic! Harrison is so helpful too!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='monkeydust on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI have a side project at the moment to use LangChain to help build a Q&A system from domain specific documentation on a product I own. Its around 500 different PDFs. I was going to follow this guide* which looks credible and solves for how to deal with large corpus of texts. Curious what others think as to approach.* https://dagster.io/blog/chatgpt-langchain'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"revskill on Jan 18, 2023  \\n             | prev | next [–] \\n\\nIt's sad that all things now has OpenAI dependency. That OpenAI is political, it's banned in some regions in the world.Academics became a shameful thing.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='fudged71 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI’ve been following both LangChain and GPT Index but to be honest I’m getting super confused which use cases are better suited for each (or a combination of both!). It would be great to see a presentation of both tools together like this :)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Mizza on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI had a similar idea, but it uses a graph construction and Elixir. Definitely going to poach some ideas from LangChain though.https://github.com/Miserlou/Helix'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='imranq on Jan 18, 2023  \\n             | prev | next [–] \\n\\nIs there support for Prompt-Tuning? This is also an interesting space to optimize inputs to frozen language modelshttps://ai.googleblog.com/2022/02/guiding-frozen-language-mo...'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"mfalcon on Jan 18, 2023  \\n             | prev | next [–] \\n\\nWhat if OpenAI decides to close or charge for access to it's API? Is there a compromise of keeping it open somewhere? What's the plan B?There's a really strong dependency on their service and I sometimes get doubtful about dedicating time to build things on top of it.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"ronsor on Jan 18, 2023  \\n             | parent | next [–] \\n\\n> OpenAI decides to close or charge for access to it's API?They already do: https://openai.com/api/pricing/\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='mbil on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nOpenAI already does charge access to its API. ChatGPT is currently free, but LLM model access has been available through a paid API since before GPT-3.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"Animats on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis looks useful. It should compete with Rasa, which is supposed to do something similar, but in practice is mostly only able to match what you're saying to nodes in a phone tree. Has anyone done a game NPC with this?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='barrenko on Jan 18, 2023  \\n             | prev | next [–] \\n\\nPretty sweet, but are there similar setup more geared towards coding?This would be a great way to get a pair programmer for people who are just coding for hobby or transitioning to SWE. Or to alleviate imposter syndrome.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='swyx on Jan 18, 2023  \\n             | parent | next [–] \\n\\ncopilot not enough for you? why?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"mark_l_watson on Jan 18, 2023  \\n             | prev | next [–] \\n\\nSuch a good idea! A few years ago I tried importing pre-trained Keras models into Racket Scheme, creating a function for each model. I didn't really get the idea right, LangChain looks much better.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='ghoshbishakh on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThank god this is not about Blockchains.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='jarbus on Jan 18, 2023  \\n             | prev | next [–] \\n\\nAhhh, I was hoping to work on something like this after I completed my PhD!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='alonger1999 on Jan 23, 2023  \\n             | prev | next [–] \\n\\nHi'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='keepquestioning on Jan 18, 2023  \\n             | prev [–] \\n\\ngamechangerhow can we use ChatGPT to design a 3d model?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"lgas on Jan 18, 2023  \\n             | parent [–] \\n\\nThis doesn't really seem like the right thread for this question, but as long as we are here... you might be interested in POINT-E https://huggingface.co/spaces/openai/point-e or Stable Dreamfusion https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUU...\")]\n","[Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 0}, page_content='Introduction:\\nProblem Environments\\n& Intelligent Agents\\nCS3243: Introduction to Artificial Intelligence – Lecture 1\\n10 January 2022'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 1}, page_content='Contents\\n1. Administrative Matters\\na. Teaching Staff\\nb. Topics & Schedule\\nc. Assessments\\nd. Resources & Textbook\\ne. Copyrights & Plagiarism\\nf. Lecture Protocols\\n2. What is AI?\\n3. Intelligent Agents\\n4. Problem Environments\\n5. Taxonomy of Agents'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 2}, page_content='Administrative Matters\\n3'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 3}, page_content='Teaching Staff\\n▪ Lecturer\\n– Daren Ler : dler@comp.nus.edu.sg\\n– https://www.comp.nus.edu.sg/cs/bio/dler/\\n▪ Tutors\\n– Bryan Wang : e0540007@u.nus.edu\\n– Eric Han : dcshlwe@nus.edu.sg\\n– Gidijala Rahul : e0425591@u.nus.edu\\n– May Lim : dcslmlm@nus.edu.sg\\n– Sagar Sureka : e0426366@u.nus.edu\\n– Teo Jia Wei : e0415724@u.nus.edu\\n4'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 4}, page_content='Topics\\n5\\n1. Introduction: Problem Environments and Intelligence Agents\\n2. Uninformed Search: Problem-solving Agents and Path Planning\\n3. Informed Search: Incorporating Domain Knowledge\\n4. Local Search: Goal Versus Path Search\\n5. Constraint Satisfaction Problems: Generalising Goal Search\\n6. Adversarial Search: Playing Games\\n7. Logical Agents: Knowledge Representation\\n8. Bayesian Networks: Representations within Uncertainty'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 5}, page_content='Weekly Schedule\\n6\\nDetailed Schedule: LumiNUS > CS3243 > Module Details > Description > Schedule\\n▪ Lectures\\n– Mondays, 1000-1200 hrs\\n▪ Diagnostic Quizzes\\n– Release: After lectures\\n– Deadlines: Fridays, 2359 hrs\\n▪ Tutorials\\n– Begin Week 3\\n– Release: After lecture\\n– Deadlines: Sunday 2359 hrs\\nNote:\\nWeek 1 deadlines \\npushed to Week 2'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 6}, page_content='General Schedule\\n7\\nDetailed Schedule: LumiNUS > CS3243 > Module Details > Description > Schedule\\n▪ Projects\\n– Project 1: released Week 3; due Week 6\\n– Project 2: released Week 6; due Week 9\\n– Project 3: released Week 9; due Week 12\\n▪ Midterm Quiz\\n– 28 February, 1010-1140 hrs (Week 7 Lecture Slot)\\n▪ Final Examination\\n– 25 April, 1700-1900 hrs'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 7}, page_content='Consultations & Other Academic Support\\n▪ Consultations\\n– By appointment only\\n– Exhaust other channels first\\n▪ LumiNUS forums\\n– Post questions in relevant forums\\n– Answer in reasonable time\\n– Do not post solutions\\n▪ Telegram groups\\n– One Telegram group per tutorial class\\n– Managed by your tutor\\n8'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 8}, page_content='Assessments\\n9\\n▪ Midterm Quiz (1)\\n– Total 20%\\n– Closed Book + Cheat Sheet\\n– Online + LumiNUS Quiz\\n– Zoom + Screen Recording\\n– NO MAKEUP\\n▪ Final Examination (1)\\n– Total 30%\\n– Closed Book + Cheat Sheet\\n– Online + LumiNUS Quiz\\n– Zoom + Screen Recording\\n▪ Diagnostic Quizzes (12)\\n– Total 10%\\n– LumiNUS Quiz (1 Attempt)\\n– 1.25% for Best 8\\n▪ Tutorial Assignments (10)\\n– Total 10%\\n– LumiNUS File Submission\\n– 1% each\\n▪ Projects (3)\\n– Total 30%\\n– Individual\\n– Each project 10% (2% competitive)'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 9}, page_content='Resources & Textbook\\n10\\n▪ LumiNUS for all course material\\n– https://luminus.nus.edu.sg/modules/e9958908-6911-453c-8e22-\\n547be8653fc6\\n– Includes link to library-archived Past Exam Papers (bottom of menu)\\n▪ Textbook\\n– Artificial Intelligence: A Modern Approach (4th Edition)\\n– IBSN 9780134610993\\n– Library: https://linc.nus.edu.sg/search/i=9780134610993\\n'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 10}, page_content='Plagiarism & Copyright \\n11\\n▪ Plagiarism\\n– F Grade in module if caught\\n– New update this week?\\n▪ Copyrights\\n'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 11}, page_content='Lecture Protocol\\n▪ Periodic pauses to take questions\\n– Unmute and ask verbally\\n– Type in Zoom chat\\n▪ Archipelago\\n– Use Voting Board\\n– Checked before Zoom chat\\n▪ Priority\\n– Verbal > Archipelago > Zoom Chat\\n▪ Lecture recordings\\n– LumiNUS > CS324 > Conferencing > Previous\\n– LumiNUS > CS3243 > Multimedia\\n12\\n'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 12}, page_content='Questions on Administrative Matters?\\n▪ Was anything unclear?\\n▪ Do you need to clarify anything?\\n▪ Channels\\n– Verbally on Zoom\\n– On Archipelago\\n– Via Zoom Chat\\n13\\n'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 13}, page_content='What is Artificial Intelligence (AI)?\\n14'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 14}, page_content='Artificial Intelligence (in a Nutshell)\\n15\\n▪ Solving problems to help humans\\n– Programs relating to human actions / thinking\\n– More dynamic solutions → able to deal with many cases\\n▪ Example\\n– Google DeepMind’s AlphaGo, AlphaZero, and MuZero\\nhttps://deepmind.com/research/case-studies/alphago-the-story-so-far (with movies)\\n– Generality of the solution is the key\\n▪ Building intelligent mechanisms \\n– Perform at least as well as humans\\n– Not necessarily in the same way as humans (nature)\\n▪ Need not follow the example we know of – e.g., birds versus planes'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 15}, page_content='Kinds of AI\\n16\\n▪ Strong AI\\n– General problem-solver\\n– Very dynamic programs → solves many problems\\n▪ Weak / Narrow AI\\n– Less dynamic programs → solves fewer problems (typically just 1)\\n– Corresponds to most AI work\\n▪ Usually focused on Narrow AI\\n– Easier to formalise\\n– More on this later …'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 16}, page_content='Intelligent Agents\\n17'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 17}, page_content='Agent Framework\\n18\\nProblem \\nEnvironment\\nAgent\\nSensors\\npercepts\\nFunction\\nActuators\\nactions'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 18}, page_content='Agent Components\\n19\\n▪ Sensors and actuators\\n– Sensors: what can/should be captured about the environment?\\n▪ Percept data at time step t, pt\\n▪ Percept sequence, P = { p1, …, pt }\\n– Actuators: how will the agent affect change in the environment?\\n▪ Set of actions, A\\n▪ Focus is on the agent function\\n– Specify a function f\\n– Such that f : P → at\\n– Where at ϵA is the selected action given P\\nCS3243 focuses on\\n• Representations for P and A\\n• Algorithms that determine f'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 19}, page_content='Rationality & Performance\\n20\\n▪ Desire a program that works well\\n– At least better than humans; ideally optimal\\n– Implies a quantifiable objective → performance measure\\n▪ Are the objectives and performance measure aligned?\\n▪ Rational agent (function), f : P → a\\n– Given\\n▪ Percept sequence\\n▪ Prior knowledge\\n▪ Set of actions\\n▪ Performance measure\\n– Rational agent optimises performance measure\\navailable data\\nNote: \\ndo not assume \\nagent is omniscient\\nWhy more Narrow AI?\\nEasier to define the performance \\nmeasure and thus a rational \\nagent to solve that problem'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 20}, page_content='AI as Search: A First Look\\n21\\n▪ Goal in AI → determine agent function f\\n– f : P → a\\n– a ∈ A\\n▪ Key idea → AI as graph search\\n– Each percept corresponds to a state\\nin the problem (state → vertex)\\n– Define the desired states → goals\\n– After each action, we arrive at a new\\nstate (action → edge)\\n– Construct a search space (graph)\\n– Design and apply a graph search algorithm\\nRecall the agent framework\\n• Agent gets percepts\\n• Agent function \\ndetermines action\\n• Agent enacts action\\n• Repeat\\nFirst problem we will look at in \\nCS3243 (next week)\\n– other topics will expand on this idea\\n(1) Define performance \\nmeasure and search space\\n(2) Design search algorithm'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 21}, page_content='An Example Agent\\n22\\n▪ Problem environment\\n– 2-dimensional maze navigation agent\\n– Pi : (row, column)\\n– A :  { ←,↑,→,↓ }\\n▪ Agent function f ?\\n– Assume map always the same\\n▪ Function: series of if statements\\n– Assume map is different each time\\nbut remains static during game\\n▪ Function: determined by path planning algorithm (e.g., Dijkstra’s)\\n– What other possible assumptions?\\n▪ We review this in the next part of the lecture\\nS       G'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 22}, page_content='Questions on Intelligent Agents?\\n▪ Was anything unclear?\\n▪ Do you need to clarify anything?\\n▪ Channels\\n– Verbally on Zoom\\n– On Archipelago\\n– Via Zoom Chat\\n24\\n'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 23}, page_content='Problem Environments\\n25'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 24}, page_content='Environment Properties\\n26\\n▪ Fully observable versus partially observable\\n– Agent cannot access all information as some cannot be sensed\\n– Requires handling uncertainty\\n▪ Deterministic verses stochastic\\n– Stochastic → intermediate state cannot be determined based on action taken \\nat a given state\\n– Handling uncertainty typically required\\n▪ Stochastic → partially observable?\\n– May be fully observable (sense all) but still have randomness with action'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 25}, page_content='Environment Properties\\n27\\n▪ Episodic versus sequential\\n– Episodic → actions only impact the current state (not those beyond)\\n– Sequential → an action may impact all future decisions\\n– Note that it is possible to model an episodic environment into a sequential \\nsearch space (more on this next week)\\n▪ Discrete versus continuous\\n– Refers to state information, time, percepts, actions'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 26}, page_content='Environment Properties\\n28\\n▪ Single vs multi-agent\\n– Do other entities exist in within the environment that are themselves agents \\nwhose actions directly influence the performance of this agent?\\n▪ Chess → opponent is a competitive agent\\n▪ Automated vehicles → other vehicles are cooperating agents\\n▪ Known versus unknown\\n– Refers to knowledge of the agent/designer (not environment itself)\\n– Includes performance measure\\n▪ Static versus dynamic\\n– Will the environment change while the agent is deciding an action?'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 27}, page_content='Environment Properties\\n29\\nProperty CS3243 Notes\\nFully / Partially Observable Both\\nLatter in Bayesian Networks\\nDeterministic / Stochastic Both\\nEpisodic / Sequential Both\\nDiscrete / Continuous Both Mostly discrete\\nSingle / Multi-agent Both Latter in Adversarial Search\\nKnown / Unknown Known\\nStatic / Dynamic Static'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 28}, page_content='Taxonomy of Agents\\n30'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 29}, page_content='Types of Agents\\n31\\n▪ Reflex agent\\n– Uses rules in the form of if-statements to make decisions\\n– Direct mapping of percepts to actions\\n– Mostly domain specific\\n– Impractical with large search spaces\\n▪ Model-based reflex agent\\n– Makes decisions based on an internalised model'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 30}, page_content='Types of Agents\\n32\\n▪ Goal-based and utility-based agents\\n– Given\\n▪ State and action representations\\n▪ Definition of goals or utility\\n– Determines\\n▪ Sequence of actions necessary to reach goals or maximise utility\\n▪ Or state that satisfies goal conditions or maximises utility\\n▪ Learning agents\\n– Agents that learn how to optimise performance'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 31}, page_content='Environment Properties\\n33\\nProperty CS3243 Notes\\nReflex Agents Yes\\nModel-Based Reflex Agents Yes • Logical Agents\\n• Bayesian Networks\\nGoal-Based and Utility-\\nBased Agents Yes\\n• Uninformed / Informed Search\\n• Local Search\\n• Constraint Satisfaction Problems\\n• Adversarial Search\\nLearning Agents No'), Document(metadata={'source': './CS3243 [2120] - Lecture 01 - v2.pdf', 'page': 32}, page_content='Questions on the Lecture?\\n▪ Was anything unclear?\\n▪ Do you need to clarify anything?\\n▪ Channels\\n– Verbally on Zoom\\n– On Archipelago\\n– Via Zoom Chat\\n34\\n')]\n"]}],"source":["# Online Loader\n","from langchain_community.document_loaders.pdf import PyPDFLoader\n","from langchain_community.document_loaders.hn import HNLoader\n","\n","# Load content from website\n","loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n","hn_data = loader.load()\n","print(hn_data)\n","\n","# Load content from local PDFs\n","loader = PyPDFLoader(\"./CS3243 [2120] - Lecture 01 - v2.pdf\")\n","pdf_data = loader.load()\n","print(pdf_data)\n","\n","# We can use a directory loader to load more than one PDF at once.\n","# loader = PyPDFDirectoryLoader(\"./\")\n","# pdf_directory_data = loader.load()"]},{"cell_type":"markdown","id":"da29979d-f779-41e1-a35c-598770a03139","metadata":{},"source":["#### Text Splitter\n","\n","**Data Chunks and Model Tokenizer**\n","\n","To efficiently handle data when building an LLM-based application, data needs to be divided in portions. Those are the so-called data chunks and the chunk size is highly determinant in the quality of the chatbot.\n","\n","The tokenizer plays a crucial role in relation to data chunks when working with LLMs: \n","- A **tokenizer is the tool used to convert text data into a format that can be processed by the model.**\n","- Data is then stored in the vector stores in the tokenized format.\n","\n","To convert the original data into tokens and split it in data chunks, we will use the **LangChain Text Splitter**.\n","\n","If you are interested in more details about the tokenizer, the article [Unleashing the ChatGPT Tokenizer](https://medium.com/towards-data-science/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54) is for you!\n"]},{"cell_type":"markdown","id":"c72a417e-7708-4d1e-bc99-0b0ba703dbe1","metadata":{},"source":["By using Langchain, we can highly customize how to split our data:\n","- **Split by chunks**: The most general approach is to split your data into chunks of a concrete size. In the following example, we will take the data that we have already loaded (`wikipedia_data`, `hn_data` and `pdf_data`) and we will split it in portions of 200 characters. \n","\n","_What will happen if the split based on character count breaks a word?_\n","\n","There is the concept of \"chunk overlap\" that refers to a method where consecutive chunks of text share some common content. This technique is used to maintain context and coherence when a long document is divided into smaller parts due to the token limitations of LLMs. In this case, we will use a chunk size of 20 characters.\n","\n","So let's split the Wikipedia data we have just loaded: "]},{"cell_type":"code","execution_count":null,"id":"4df0ebc1-87a3-4755-a05d-4088db03b29d","metadata":{"executionCancelledAt":null,"executionTime":403,"lastExecutedAt":1704818041060,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Advanced method - Split by chunks ________________________________________________________________________\n# Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\nprint(\"\\nSPLITTING BY CHUNKS\")\nwikipedia_chunks = text_splitter.split_documents(wikipedia_data)\nprint(\"Wikipedia Data - Now you have {0} number of chunks.\".format(len(wikipedia_chunks)))","outputsMetadata":{"0":{"height":80,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":77,"type":"stream"},"4":{"height":77,"type":"stream"},"5":{"height":57,"type":"stream"},"6":{"height":77,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","SPLITTING BY CHUNKS\n","Wikipedia Data - Now you have 78 number of chunks.\n"]}],"source":[]},{"cell_type":"markdown","id":"2cb800f3-0b01-4bff-9291-1736cb5cdbe4","metadata":{},"source":["**TASK:**\n","\n","Generate the chunks for both `HNLoader` and `PyPDFLoader`. \n","1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n","2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","3. Define a count_tokens function that will allow us to count the tokens of out text. \n","4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n","5. Apply the command `.split_documents`to our data. \n","\n","You can define your own function for the HN data and use your the default function for the PDF Data.  "]},{"cell_type":"code","execution_count":null,"id":"bd61a742-dc5e-441a-a746-37d43012aceb","metadata":{"executionCancelledAt":null,"executionTime":288,"lastExecutedAt":1704818288170,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#_____________________________________________________________________PDF DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n \n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=20,\n    length_function=count_tokens\n)\n\n# 5 - Apply the .split_document command\npdf_chunks = text_splitter.split_documents(pdf_data)\nprint(\"PDF Data - Now you have {0} number of chunks.\".format(len(pdf_chunks)))\n\n\n#_____________________________________________________________________HN DATA\n# 3 - We use the default len, no need to do anything.\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=20\n)\n\nhn_chunks = text_splitter.split_documents(hn_data)\nprint(\"Online HN - Now you have {0} number of chunks.\".format(len(hn_chunks)))","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["PDF Data - Now you have 64 number of chunks.\n","Online HN - Now you have 232 number of chunks.\n"]}],"source":[]},{"cell_type":"markdown","id":"ad447dd1-d5a5-4b2e-91ce-ef41539ddf73","metadata":{},"source":["We can make sure that the chunking has been successful by visualizing the distribution of chunk sizes. \n","Since we have selected a chunk size of 200, the majority of our chunks should have this lenght:"]},{"cell_type":"code","execution_count":null,"id":"31777cc6-9a05-45d6-b1b6-01db6587270c","metadata":{"executionCancelledAt":null,"executionTime":309,"lastExecutedAt":1704818302589,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Quick data visualization to ensure chunking was successful\n\n# Create a list of token counts\ntoken_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n\n# Create a DataFrame from the token counts\ndf = pd.DataFrame({'Token Count': token_counts})\n\n# Create a histogram of the token count distribution\ndf.hist(bins=40, )\n\n# Show the plot\nplt.show()"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":[]},{"cell_type":"markdown","id":"c149bde7-39f5-4912-83f2-fb238e4d72a7","metadata":{},"source":["- **Split by pages**: If your data comes from documents organized in pages, there are methods that allow you to split data in pages to keep track of the page content. This method is specially useful when dealing with PDFs, as in the following example:"]},{"cell_type":"code","execution_count":null,"id":"87a8e84c-d658-4a7a-8144-d84fcb169928","metadata":{"executionCancelledAt":1704727738639,"executionTime":1063,"lastExecutedAt":1704708532500,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Simple method - Split by pages    ________________________________________________________________________\n# You need a PDF file in your environement. \nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_pages_chunks = loader.load_and_split()\npdf_pages_chunks\n\nprint(\"\\nSPLITTING BY PAGES\")\nprint(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))","outputsMetadata":{"0":{"height":77,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":137,"type":"stream"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"06933573-24eb-4ba2-bf05-4410912be27e","metadata":{},"source":["### Vector Stores\n","\n","Vector stores, also known as vector databases, are specialized types of databases designed to efficiently handle and manipulate high-dimensional vector data. In our case, we will store the tokenized and splitted content, e.g., the data chunks in the format that LLMs can process.\n","\n","There are different types of vector stores. Depending on the storage of the data, we can classify them as:\n","- **Local Vector Stores**: This type of databases store the information in your local system. As an example of Local Vector Store, we will use FAISS.\n","- **Online Vector Stores**: This type of databases store the information in the cloud. We will use Pinecone as out preferred option for Online Vector Stores.\n","\n","FAISS - EXAMPLE OF LOCAL VECTOR STORE"]},{"cell_type":"code","execution_count":null,"id":"b7dc17d0-275f-4376-86e1-11661a2dd1ef","metadata":{"executionCancelledAt":null,"executionTime":4673,"lastExecutedAt":1704818404491,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.vectorstores import FAISS  # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Get embedding model\nembeddings = OpenAIEmbeddings()\n\n# OPTION 1: FAISS (Facebook AI Similarity Search) Local _______________________________________________________________________________________\n# Create vector database\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)","outputsMetadata":{"0":{"height":580,"type":"stream"},"1":{"height":616,"type":"stream"}}},"outputs":[],"source":[]},{"cell_type":"markdown","id":"1c4faca6-6ae9-453c-90d0-b31fc75a7872","metadata":{},"source":["PINECONE - EXAMPLE OF ONLINE VECTOR STORE"]},{"cell_type":"code","execution_count":null,"id":"776ae3bd-a6d2-4066-881a-dd08cc9f9ede","metadata":{"executionCancelledAt":null,"executionTime":2496,"lastExecutedAt":1704818462367,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pinecone  #We need the Pinecone library to initialize our connection.\nfrom langchain.vectorstores import Pinecone # for the vector database part -- Pinecone is cloud-\n# OPTION 2: PINECONE Online\n     \n# We initialize pinecone\npinecone.init(      \n\tapi_key=os.getenv(\"PINECONE_API_KEY\"),      \n\tenvironment=os.getenv(\"PINECONE_ENV_KEY\")     \n) \n\n# Create a new pinecone index\n#pinecone.create(name=\"langchain\", dimension=1536, metric=\"cosine\")\n\n# We define the name of our index (in case the index is already created)\nindex_name = \"langchain\"\n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\ndb_Pinecone = Pinecone.from_documents(pdf_chunks, embeddings, index_name=index_name)"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e2e6151d-08d3-4c8f-8a7a-643c101bd701","metadata":{},"source":["### Natural Language Retrieval\n","We first start performing a semantic search within our Vector DataBase. "]},{"cell_type":"code","execution_count":null,"id":"826da9fb-03ba-4101-b793-3b9ac3203d79","metadata":{"executionCancelledAt":null,"executionTime":900,"lastExecutedAt":1704818532774,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=2)\nprint(matches)","outputsMetadata":{"0":{"height":437,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com', metadata={'source': 'Docs/attentions.pdf', 'page': 0, 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com'}), Document(page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'Docs/attentions.pdf', 'page': 13, 'text': '.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'})]\n"]}],"source":[]},{"cell_type":"markdown","id":"17489dca-2a53-40ab-9b12-13a65e1f8c53","metadata":{},"source":["In the above section, we have seen how to retrieve the coincidences of you query in the documents in our vector store. Nevertheless, the output is a bit difficult to read. We can leverage the usage of LLMs by feeding the coincidences in our vector store to an LLM and let it generate a response in Natural Language using the additional information from our documents. We can do so by using the so-called **[LangChain Chains](https://python.langchain.com/docs/expression_language/get_started)**."]},{"cell_type":"code","execution_count":null,"id":"ccc9684f-f666-473f-8a48-9c8f53e413ab","metadata":{"executionCancelledAt":null,"executionTime":2981,"lastExecutedAt":1704818563496,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# ONLINE - PINECONE\n\n# 1. Define our query of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 2. Perform the semantic search in our vector database with the similarity_search command.  \nmatches = db_Pinecone.similarity_search(query, k=2)\n\n# 3. Define a load_qa_chain.\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Execute the chain with the prompt and the matches. \nchain.run(input_documents=matches, question = query)","outputsMetadata":{"0":{"height":257,"type":"stream"}}},"outputs":[{"data":{"text/plain":["'The authors of the article \"Attention Is All You Need\" are:\\n\\n1. Ashish Vaswani\\n2. Noam Shazeer\\n3. Niki Parmar\\n4. Jakob Uszkoreit\\n5. Llion Jones\\n6. Aidan N. Gomez\\n7. Łukasz Kaiser\\n8. Illia Polosukhin'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"markdown","id":"812cbc3f-2d98-4085-8fa4-827cf7589e24","metadata":{},"source":["**TASK:**\n","Repeat the previous procedure using FAISS.\n","1. Define a query of your interest. For example, \"Can you please tell me all the autors of the article Attention is all you need?\"\n","2. Use the db_Pinecone database together with the `.similarity_search`command to perform a semantic search. \n","3. Define a `load_qa_chain`and pass the matches together with the query to obtain a NLP based answer. "]},{"cell_type":"code","execution_count":null,"id":"d164cac5-2244-4582-b567-e9ae4461afdd","metadata":{"executionCancelledAt":null,"executionTime":2611,"lastExecutedAt":1704818697699,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=4)\n\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\nchain.run(input_documents=matches, question = query)\n"},"outputs":[{"data":{"text/plain":["'The authors of the article \"Attention Is All You Need\" are:\\n\\n1. Ashish Vaswani\\n2. Noam Shazeer\\n3. Niki Parmar\\n4. Jakob Uszkoreit\\n5. Llion Jones\\n6. Aidan N. Gomez\\n7. Łukasz Kaiser\\n8. Illia Polosukhin'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"markdown","id":"95ad7d06-178b-467b-979b-8976582012a7","metadata":{},"source":["### Indexes and Metadata\n","When we upload data to our vector database, there is metadata that allows us to understand where the data is coming from. \n","When dealing with PDFs, the source information allows us to know what pdf and page the info is coming from."]},{"cell_type":"code","execution_count":null,"id":"97d1db0b-b343-4836-b43b-8e4ab735c033","metadata":{"executionCancelledAt":null,"executionTime":174,"lastExecutedAt":1704818705089,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#from langchain.embeddings import OpenAIEmbeddings\n#from langchain.indexes import index\n\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\n\nprint(\"______________________________________ THIRD MATCH\")\n\nprint(\"We can get the chunk text content and get: \\n\", matches[3].page_content)\nprint(\"\\nWe can get the chunk metadata and get: \\n\", matches[3].metadata)\nprint(\"\\nThe source of our match is: \\n\" , matches[3].metadata[\"source\"], \"and page\", matches[3].metadata[\"page\"])","outputsMetadata":{"0":{"height":542,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["______________________________________ THIRD MATCH\n","We can get the chunk text content and get: \n"," our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n","training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n","best models from the literature. We show that the Transformer generalizes well to\n","other tasks by applying it successfully to English constituency parsing both with\n","large and limited training data.\n","∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n","the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n","has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n","attention and the parameter-free position representation and became the other person involved in nearly every\n","detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n","\n","We can get the chunk metadata and get: \n"," {'source': 'Docs/attentions.pdf', 'page': 0, 'text': 'our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'}\n","\n","The source of our match is: \n"," Docs/attentions.pdf and page 0\n"]}],"source":[]},{"cell_type":"markdown","id":"f2e5f032-6e4d-40b2-b851-ccb9c86fc097","metadata":{},"source":["Now it is the time to put it all together and generate a simple pipeline to query our documents using a LLM model. "]},{"cell_type":"markdown","id":"4d0282a5-73c9-44b8-87fd-eb175d676748","metadata":{},"source":["# PART 2: Loading and processing our documents\n","\n","\n","\n","PyPDFDirectoryLoader allows us to upload multiple PDFs at once. In our case, we have two PDFs in the Docs directory."]},{"cell_type":"markdown","id":"89db047f-2475-4738-a726-458a5799a21e","metadata":{},"source":["## **STEP 1 - LOADER**\n","\n","Use the `PDFDirectoryLoader` to upload all PDFs contained within the the Docs folder. "]},{"cell_type":"code","execution_count":null,"id":"a374a538-47c8-4679-a5d6-6c3e4a8a208c","metadata":{"executionCancelledAt":null,"executionTime":13294,"lastExecutedAt":1704818828591,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#STEP 1 - LOADER\nfrom langchain.document_loaders import PyPDFDirectoryLoader\n\nloader = PyPDFDirectoryLoader(\"Docs/\")\n\ndata = loader.load()"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"d8aebdc5-1686-4401-89d2-3b59546f8d3c","metadata":{},"source":["## **STEP 2 - CHUNKING**\n","\n","Generate the chunks for the PDFs contained in the directory. \n","1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n","2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","3. Define a count_tokens function that will allow us to count the tokens of out text. \n","4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n","5. Apply the command `.split_documents`to our data. "]},{"cell_type":"code","execution_count":null,"id":"198312a1-bd33-4922-b5fd-1e402cd10c91","metadata":{"executionCancelledAt":null,"executionTime":1378,"lastExecutedAt":1704818978129,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#STEP 2 - CHUNKING OUR DATA\n#_____________________________________________________________________PDFs Data\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\nchunks = text_splitter.split_documents(data)\nprint(\"Multiple PDFs - Now you have {0} number of chunks.\".format(len(chunks)))","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":38,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["Multiple PDFs - Now you have 610 number of chunks.\n"]}],"source":[]},{"cell_type":"markdown","id":"8f548885-aee9-4b4a-bf1d-e996ed02c74d","metadata":{},"source":["## **STEP 3 - EMBEDD AND UPLOAD THE DATA INTO A VECTORSTORE**\n","\n","**TASK**\n","- Upload the data into the FAISS vector store using the `from_documents`command. "]},{"cell_type":"code","execution_count":null,"id":"04a6a94c-153d-438a-978b-df189a33fa6c","metadata":{"executionCancelledAt":null,"executionTime":4211,"lastExecutedAt":1704819033112,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 3 - EMBEDDING AND UPLOAD DATA INTO OUR VECTORSTORE\n\n# ___________________________________________________________________________ LOCAL VERSION\n\n# 1. Create vector database with FAISS\ndb_FAISS = FAISS.from_documents(chunks, embeddings)\n\n# Check similarity search is working\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\nprint(\"We found {0} number of similarities.\".format(len(matches)))\nfor match in matches:\n    print(\"\\n\", match.page_content)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["We found 4 number of similarities.\n","\n"," To the best of our knowledge, however, the Transformer is the first transduction model relying\n","entirely on self-attention to compute representations of its input and output without using sequence-\n","aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n","self-attention and discuss its advantages over models such as [17, 18] and [9].\n","3 Model Architecture\n","Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n","Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n","of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n","sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n","\n"," vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior\n","works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al.,\n","2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but\n","these mostly focus on code retrieval, classiﬁcation, and program repair. Several recent and concurrent\n","efforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al.,\n","2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they\n","focus on generating code in a single turn, we propose to factorize the speciﬁcations into multiple turns\n","\n"," The Transformer allows for significantly more parallelization and can reach a new state of the art in\n","translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n","2 Background\n","The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n","[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n","block, computing hidden representations in parallel for all input and output positions. In these models,\n","the number of operations required to relate signals from two arbitrary input or output positions grows\n","in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n","it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n","reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n","\n"," illia.polosukhin@gmail.com\n","Abstract\n","The dominant sequence transduction models are based on complex recurrent or\n","convolutional neural networks that include an encoder and a decoder. The best\n","performing models also connect the encoder and decoder through an attention\n","mechanism. We propose a new simple network architecture, the Transformer,\n","based solely on attention mechanisms, dispensing with recurrence and convolutions\n","entirely. Experiments on two machine translation tasks show these models to\n","be superior in quality while being more parallelizable and requiring significantly\n","less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n","to-German translation task, improving over the existing best results, including\n","ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n"]}],"source":[]},{"cell_type":"markdown","id":"37afb3bf-6c25-4562-b9c7-f32d17729cf9","metadata":{},"source":["# PART 3: Talking with our documents"]},{"cell_type":"markdown","id":"b5d54ac1-c3ff-42c3-a9c8-151548914489","metadata":{},"source":["## STEP 4 - DEFINE A CHAIN AND PERFORM THE SIMILARITY SEARCH\n","Generating a simple pipeline to query our documents with a load_qa_chain. \n","**TASK**\n","1. Import the `load_qa_chain`from the langchain.chains.question_answering library. \n","2. Define a prompt of interest, like: \"Can you please tell me all the autors of the article Attention is all you need?\"\n","3. Define the chain.\n","4. Perform a semantic search with the `.similarity_search`. \n","5. Execute the chain. "]},{"cell_type":"code","execution_count":null,"id":"85627ed4-89b0-475c-b7ea-caba04fce893","metadata":{"executionCancelledAt":null,"executionTime":1844,"lastExecutedAt":1704819185918,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. Execute the chain to obtain a NLP based response. \nresponse = chain.run(input_documents = matches, question = query)\nprint(response)","outputsMetadata":{"0":{"height":227,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["The authors of the article \"Attention Is All You Need\" are:\n","\n","1. Ashish Vaswani\n","2. Noam Shazeer\n","3. Niki Parmar\n","4. Jakob Uszkoreit\n","5. Llion Jones\n","6. Aidan N. Gomez\n","7. Łukasz Kaiser\n","8. Illia Polosukhin\n"]}],"source":[]},{"cell_type":"markdown","id":"3570da7e-07ca-4dbd-a390-0f74aeabc044","metadata":{},"source":["Now that we already have a working pipeline to query our documents, we want to understand where our data is coming from. "]},{"cell_type":"code","execution_count":null,"id":"c9445c4e-6c17-4a2f-8a5e-0456873c8ac6","metadata":{"executionCancelledAt":null,"executionTime":3750,"lastExecutedAt":1704819277172,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","outputsMetadata":{"0":{"height":269,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["The authors of the article \"Attention Is All You Need\" are:\n","\n","1. Ashish Vaswani - Google Brain (source: attentions.pdf, page 0)\n","2. Noam Shazeer - Google Brain (source: attentions.pdf, page 0)\n","3. Niki Parmar - Google Research (source: attentions.pdf, page 0)\n","4. Jakob Uszkoreit - Google Research (source: attentions.pdf, page 0)\n","5. Llion Jones - Google Research (source: attentions.pdf, page 0)\n","6. Aidan N. Gomez - University of Toronto (source: attentions.pdf, page 0)\n","7. Łukasz Kaiser - Google Brain (source: attentions.pdf, page 0)\n","8. Illia Polosukhin - (source: attentions.pdf, page 0)\n","\n","Please note that the information provided is based on the given context.\n"]}],"source":[]},{"cell_type":"markdown","id":"09cfc58e-6995-4c1d-85b6-1cf185d480df","metadata":{},"source":["Try to ask the model something that is completely out of scope, and see what happens!"]},{"cell_type":"code","execution_count":null,"id":"7812e8d8-e650-4444-b616-36834abe6c23","metadata":{"executionCancelledAt":null,"executionTime":1558,"lastExecutedAt":1704819297846,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"What are the main problems to cook with olive oil?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["I'm sorry, but I don't have the information you're looking for. The provided information is from a PDF document titled \"codegen.pdf\" on page 5.\n"]}],"source":[]},{"cell_type":"markdown","id":"e9aeec8e-842f-4505-b895-8818cc7021fb","metadata":{},"source":["Try other queries and talk with your documents!"]},{"cell_type":"code","execution_count":null,"id":"9c4b26ec-dc92-49bc-9955-2516e8a0cca6","metadata":{"executionCancelledAt":1704727738652,"executionTime":49,"lastExecutedAt":1704708572393,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def asking_your_model(query, k):\n    # Define the chain\n    chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n    #Perform a similarity search. \n    matches = db_FAISS.similarity_search(query, k=k)\n    #We define both the text and the metadata obtain from the semantic search.\n    input_text = [x.page_content for x in matches]\n    input_metadata= [x.metadata for x in matches]\n    #We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n    meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in       the response\".format(input_metadata) \n    #We define an enriched query with the initial prompt and the metadata prompt. \n    enriched_query = query + meta_data_enriching\n    #We execute the chain. \n    response = chain.run(input_documents = matches, question = enriched_query)\n    return response\n    "},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"64a2c499-6ef6-424d-b58d-fbd571dd0522","metadata":{"executionCancelledAt":1704727738653,"executionTime":2198,"lastExecutedAt":1704708574592,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is functional correctness?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":97,"type":"stream"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e6eb01de-7644-45a0-a88c-f290ee351640","metadata":{"executionCancelledAt":1704727738654,"executionTime":3163,"lastExecutedAt":1704708577755,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is the multi-head attention in a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":217,"type":"stream"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0925bf69-fc5b-47d8-8f7d-322dab181ae3","metadata":{"executionCancelledAt":1704727738655,"executionTime":3217,"lastExecutedAt":1704708580972,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What are the main components of a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":137,"type":"stream"}}},"outputs":[],"source":[]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
