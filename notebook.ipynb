{"cells":[{"cell_type":"markdown","id":"bdd098e2-49c6-43df-9151-050694970b75","metadata":{},"source":["<span style=\"font-size: 25px;\">**Chat with Your Documents Using GPT & LangChain**</span>\n","\n","\n","**Objectives:** \n","- *Learn how to effectively load & store documents using LangChain*\n","- *Build a retrieval augmented generation pipeline for querying data*\n","- *Build a question-answering bot that answers questions based on your documents*\n","\n","You can learn more about the LangChain library in the following links:\n","* [How to Make Large Language Models Play Nice with Your Software Using LangChain](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)\n","* [6 Problems of LLMs That LangChain is Trying to Assess](https://www.kdnuggets.com/6-problems-of-llms-that-langchain-is-trying-to-assess)\n","\n","Let's start by understanding our main goal:\n","\n","First: \n","- Take a set of PDFs. \n","- Break them into pieces of texts. \n","- Embed them into a vectorized representation. \n","- Store them into a vector database. (FAISS, CHROMA, PINECONE...)\n","- Once the vectors are persistend in the ddbb, we can get queries, embed them and find a similar chunk vectors. \n","- The chunks are ranked according to how relevant they are to the question and are used to contextualize our LLM. \n","\n","**IMPORTANT:** The LLM doesn't really know what PDFs have. We take advantage of the LLM model to generate NLP answers and provide it with a question and a context to generate an accurate answer. \n","\n","![Structure_main](Structure_main.png)"]},{"cell_type":"markdown","id":"42239f47-628c-4f50-aa9d-ee482177dd0c","metadata":{},"source":["## 1. Install Imports and API Keys\n","\n","We need to make sure our environment has the following packages. "]},{"cell_type":"code","execution_count":20,"id":"7430d0f5-7341-464e-8d2e-c1f322edeb76","metadata":{"executionCancelledAt":null,"executionTime":53044,"lastExecutedAt":1709641405903,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install openai==0.27.1\n!pip install langchain==0.0.184\n!pip install tiktoken\n!pip install wikipedia\n!pip install pypdf\n!pip install faiss-cpu\n!pip install pinecone-client","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --quiet openai\n","%pip install --quiet langchain\n","%pip install --quiet tiktoken\n","%pip install --quiet wikipedia\n","%pip install --quiet pypdf\n","%pip install --quiet faiss-cpu\n","%pip install --quiet pinecone-client\n","%pip install --quiet pandas\n","%pip install --quiet matplotlib\n","%pip install --quiet python-dotenv\n","%pip install --quiet transformers\n","%pip install --quiet langchain_openai\n","%pip install --quiet langchain_community\n","%pip install --quiet langchain_text_splitters\n","%pip install --quiet langchain_openai\n","%pip install --quiet langchain-pinecone\n","%pip install --quiet langchain-pinecone\n","%pip install --quiet langchain_core\n","%pip install --quiet langgraph\n","%pip install --quiet langchain_anthropic"]},{"cell_type":"markdown","id":"26db4425-1415-4422-878e-fc555b2fbe0b","metadata":{},"source":["Before starting, make sure you have avaiable: \n","- OpenAI API Key\n","- Pinecone API Key and environment. \n","\n","To get our API keys, we can set them in an `.env` document and load them into our environement using the `load_dotenv()` command or define them directly. \n","- To obtain OpenAI API Keys, you can follow the instructions [here](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197). \n","- To obtain Pinecone API keys, you can follow the instructions [here](https://medium.com/forcodesake/pinecone-api-chatgpt-artificial-intelligence-4332de128dd5). "]},{"cell_type":"code","execution_count":2,"id":"0d8360cb-c7a0-4976-8fe9-5482d14b4495","metadata":{"executionCancelledAt":null,"executionTime":1591,"lastExecutedAt":1704816818752,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Basics\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\n\n# LangChain Training\n# LLM\nfrom langchain.llms import OpenAI\n\n# Document Loader\nfrom langchain.document_loaders import PyPDFLoader \n\n# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Embedding\nfrom langchain.embeddings import OpenAIEmbeddings \n\n# Vector DataBase\nfrom langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent. \n\n# Chains\n#from langchain.chains.question_answering import load_qa_chain\n#from langchain.chains import ConversationalRetrievalChain"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/domszy/Desktop/Document Interaction with GPT and Langchain/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"]}],"source":["# Basics\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from dotenv import load_dotenv\n","import langchain_core\n","\n","# LangChain Training\n","# LLM, embeddings\n","from langchain_openai import OpenAI, OpenAIEmbeddings\n","\n","# Document Loader\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# Splitter\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","# Tokenizer\n","from transformers import GPT2TokenizerFast\n","\n","# Vector DataBase\n","# for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent.\n","from langchain_community.vectorstores import FAISS\n","from langchain_pinecone import PineconeVectorStore\n","\n","# Chains\n","# from langchain.chains.question_answering import load_qa_chain\n","# from langchain.chains import ConversationalRetrievalChain"]},{"cell_type":"code","execution_count":3,"id":"f0d70449-d284-43a6-ae31-df6e62ca1302","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1704816847276,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# We can directly upload our keys using a .env\n#load_dotenv()\n\nimport os\n\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\npinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\npinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n\n# Alternatively, you can set the API keys as follows:\n#OPENAI_API_KEY   = \"sk-\"\n#PINECONE_API_KEY = \"34...\"\n#PINECONE_ENV_KEY = \"gcp-starter\""},"outputs":[],"source":["# We can directly upload our keys using a .env\n","import os\n","load_dotenv()\n","\n","\n","openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n","pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n","pinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n","\n","# Alternatively, you can set the API keys as follows:\n","# OPENAI_API_KEY   = \"sk-\"\n","# PINECONE_API_KEY = \"34...\"\n","# PINECONE_ENV_KEY = \"gcp-starter\""]},{"cell_type":"markdown","id":"8a853ff5-0446-4f29-8e78-66fbe290c7f9","metadata":{},"source":["\n","# PART 1: LANGCHAIN BASICS\n","\n","\n","🎯 **Objective:** Understand what is the LangChain library and all the elements that are required to generate a simple pipeline to query out documents. \n","\n","### **What is LangChain?**\n","> LangChain is a framework for developing applications powered by language models.\n","\n","LangChain makes the hardest parts of working with AI models easier in two main ways:\n","\n","1. **Data-aware** - Bring external data, such as your files, other applications, and API data, to your LLMs\n","2. **Agentic** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next. \n","\n","### **Why LangChain?**\n","1. **Components** - Abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n","\n","2. **Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together. A structured assembly of components for accomplishing specific higher-level tasks.\n","\n","3. **Speed 🚢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n","\n","4. **Community 👥** - Wonderful discord and community support, meet ups, hackathons, etc.\n","\n","Though the usage of LLMs can be straightforward (text-in, text-out), when trying to build complex applications you'll quickly notice friction points. \n","\n","> LangChain helps with once you develop more complicated application and manage LLMs the way we want. "]},{"cell_type":"markdown","id":"1f548002-dda8-4d89-9c1e-7057392c11c5","metadata":{},"source":["## LangChain Components\n","\n","The LangChain library contains multiple elements to ease the process of building complex applications using LangChain.\n","In this module we will focus mainly in 10 elements:\n","\n","**To load and process our documents**\n","- Document Loaders\n","- Text Splitters\n","- Chat Messages *(Optional)*\n","\n","\n","**To talk with our documents using NLP**\n","- LLM model (GPT, Llama...)\n","- Chains\n","- Natural Language Retrieval\n","- Metadata and Indexes\n","- Memory *(Optional)*\n","\n","**Both Processes**\n","- Text Embedding (OpenAI or Open-source models)\n","- Vector Stores \n","\n","![Structure_basics](Structure_basics.png)\n"]},{"cell_type":"markdown","id":"46cb81ae-fe1d-4108-a050-2e66c1bbf296","metadata":{},"source":["###  **The Model - Large Language Model of our choice**\n","An AI-powered LLM that takes text in and responses text out. \n","The default model is always ada-001, but we can explicitly choose the model of our preference. \n","\n","You can check the list of all avaialble models [here](https://platform.openai.com/docs/models)"]},{"cell_type":"code","execution_count":9,"id":"f1c96d08-9a35-40f5-a248-f0cb74a842fc","metadata":{"executionCancelledAt":null,"executionTime":4360,"lastExecutedAt":1704817021400,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.llms import OpenAI\n\nchatgpt = OpenAI(\n                 model_name = \"gpt-3.5-turbo\", \n                 temperature= 0\n)\n\nprompt=\"Please, tell me some funny jokes\"\n\nprint(chatgpt(prompt))","outputsMetadata":{"0":{"height":500,"type":"stream"},"1":{"height":437,"type":"stream"}}},"outputs":[{"data":{"text/plain":["\"\\n\\n1. Why don't scientists trust atoms? Because they make up everything.\\n\\n2. What do you call a fake noodle? An impasta.\\n\\n3. Why couldn't the bicycle stand up by itself? Because it was two-tired.\\n\\n4. What do you call an alligator in a vest? An investi-gator.\\n\\n5. Why did the tomato turn red? Because it saw the salad dressing.\\n\\n6. What do you call a belt made out of watches? A waist of time.\\n\\n7. How do you organize a space party? You planet.\\n\\n8. Why did the scarecrow win an award? Because he was outstanding in his field.\\n\\n9. What do you call a fish wearing a bowtie? Sofishticated.\\n\\n10. Why did the math book look sad? Because it had too many problems.\\n\\n11. What do you call a bear with no teeth? A gummy bear.\\n\\n12. Why did the chicken go to the seance? To get to the other side.\\n\\n13. What do you call a sleeping dinosaur? A dino-snore.\\n\\n14. Why did the tomato turn red? Because it saw the salad dressing.\\n\\n15. What do you call a belt made out of watches? A waist of time.\\n\\n\""]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","\n","model = OpenAI(\n","    model=\"gpt-3.5-turbo-instruct\",\n","    temperature=0\n",")\n","\n","# using prompt templates in this case\n","prompt_template = PromptTemplate.from_template(\n","    template=\"Please, tell me some funny jokes\"\n",")\n","\n","completion_chain = prompt_template | model\n","\n","completion_chain.invoke({})"]},{"cell_type":"markdown","id":"e039302b-02bb-499c-8637-f4be1bd2e34c","metadata":{},"source":["### **Chat Messages**\n","LangChain allows us to segmentate prompts into three main types.(System, Human, AI)\n","\n","* **System** - Helpful background context that tell the AI its high-level behavior.\n","* **Human** - Messages that represent the user input. \n","* **AI** - Messages that show the response of the AI model, they work as examples to the model. \n","\n","\n","For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"]},{"cell_type":"code","execution_count":12,"id":"5a6c86bf-542a-43f3-a1e7-221ed9bf6918","metadata":{"executionCancelledAt":null,"executionTime":954,"lastExecutedAt":1704817116502,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nhigh_level_behavior = \"\"\"\n                       You are an AI bot that help people decide where to travel. \n                       Always recommend three destination with a short sentence for each.\n                      \"\"\"\n\nresponse = chatgpt(\n    [\n        SystemMessage(content=high_level_behavior),\n        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n        HumanMessage(content=\"Where should I travel next?\"),\n    ]\n)\n\nprint(response.content)","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":57,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Here are three destination recommendations for you:\n","\n","1. Bali, Indonesia - Explore stunning beaches, lush rice terraces, and vibrant cultural attractions in this tropical paradise.\n","2. Barcelona, Spain - Immerse yourself in the vibrant art, architecture, and culinary scene of this dynamic city by the Mediterranean Sea.\n","3. Banff National Park, Canada - Experience the breathtaking beauty of the Canadian Rockies with crystal-clear lakes, towering mountains, and abundant wildlife.\n"]}],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","\n","model = ChatOpenAI(\n","    temperature=0,\n","    max_tokens=1000\n",")\n","\n","high_level_behavior = \"\"\"You are an AI bot that help people decide where to travel. Always recommend three destination with a short sentence for each.\"\"\"\n","\n","response = model.invoke(\n","    [\n","        SystemMessage(content=high_level_behavior),\n","        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n","        HumanMessage(content=\"Where should I travel next?\"),\n","    ]\n",")\n","\n","print(response.content)"]},{"cell_type":"markdown","id":"ab221be1-08fd-4942-b06e-5945cce7b148","metadata":{},"source":["You can also pass more chat history with responses from the AI"]},{"cell_type":"code","execution_count":null,"id":"035d9b68-ff7c-48fe-98e4-76961df7c480","metadata":{"executionCancelledAt":null,"executionTime":3077,"lastExecutedAt":1704817121541,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"response = chatgpt(\n        [\n            SystemMessage(content=high_level_behavior),\n            AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n            HumanMessage(content=\"Where should I travel next?\"),\n            SystemMessage(content=\"What do you enjoy doing?\"),\n            HumanMessage(content=\"I love going to Museums?\"),\n        ]\n    )\n\nprint(response.content)","outputsMetadata":{"0":{"height":227,"type":"stream"},"1":{"height":217,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Based on your love for museums, here are three destinations you might enjoy:\n","\n","1. Paris, France: Known as the \"City of Museums,\" Paris is home to world-renowned museums like the Louvre, Musée d'Orsay, and Centre Pompidou, offering a rich collection of art, history, and culture.\n","\n","2. Florence, Italy: Florence is a treasure trove of art and history, with museums like the Uffizi Gallery and Accademia Gallery housing masterpieces by Michelangelo, Botticelli, and more, making it a haven for art enthusiasts.\n","\n","3. St. Petersburg, Russia: St. Petersburg boasts the Hermitage Museum, one of the largest and most prestigious art museums in the world, showcasing a vast collection of art and cultural artifacts from around the globe.\n"]}],"source":["response = model(\n","    [\n","        SystemMessage(content=high_level_behavior),\n","        AIMessage(\n","            content=\"Hello! I am a traveller assistant, how can I help you?\"),\n","        HumanMessage(content=\"Where should I travel next?\"),\n","        SystemMessage(content=\"What do you enjoy doing?\"),\n","        HumanMessage(content=\"I love going to Museums?\"),\n","    ]\n",")\n","\n","print(response.content)"]},{"cell_type":"markdown","id":"cf8f2cdd-0466-423f-8984-45a5e54585af","metadata":{},"source":["### **Text Embedding Model**\n","\n","When documents or string-variables are too long, things can get quite complicated. \n","\n","**In order to be able to process them, we can embed and convert string variables into vectors** (a series of numbers that hold the semantic 'meaning' of your text).\n","\n","Mainly used when comparing different pieces of text or when dealing with huge texts. "]},{"cell_type":"markdown","id":"fd391e69-5df4-4de4-8625-c590fb372227","metadata":{},"source":["**TASK:**\n","- First import the `Embeddings` model from langcgain.embeddings.\n","- Define a text to embed. \n","- Embed the text with the `.embed_query` command. "]},{"cell_type":"code","execution_count":13,"id":"3980f040-5612-4b0b-9a2d-3b9e3c8b2ba4","metadata":{"executionCancelledAt":null,"executionTime":157,"lastExecutedAt":1704817207570,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the embedding model\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 2. Create an instance of the model\nembeddings = OpenAIEmbeddings()\n\n# 3. Define a text to embed\ntext = \"This is a webinar!\"\n\n# 4. Embed the text\ntext_embedding = embeddings.embed_query(text)\n\nprint (f\"Your embedding is length {len(text_embedding)}\")\nprint (f\"Here's a sample: {text_embedding[:5]}...\")","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Your embedding is length 1536\n","Here's a sample: [-0.0022637732326984406, -0.003931235522031784, -0.0011279426980763674, -0.01740344613790512, -0.021252648904919624]...\n"]}],"source":["# 1. Create an instance of the model\n","embeddings = OpenAIEmbeddings()\n","\n","# 2. Define a text to embed\n","text = \"Hi! It's time to go to a Museum!\"\n","\n","# 3. Embed the text\n","text_embedding = embeddings.embed_query(text)\n","print(f\"Your embedding is length {len(text_embedding)}\")\n","print(f\"Here's a sample: {text_embedding[:5]}...\")"]},{"cell_type":"markdown","id":"35ae7808-4897-4435-aa62-b29625f0c5aa","metadata":{},"source":["### Memory\n","When interacting with a model, it is important to keep track of all interactions performed with it. LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n","\n","It is important to consider that storing all the interactions with the model can quickly escalate to a considerable amount of tokens to process every time we prompt the model. It is essential to bear in mind that ChatGPT has a token limit per interaction.\n","\n","You can learn more about memory [here]([https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/])"]},{"cell_type":"code","execution_count":10,"id":"e6dd11b7-b05e-435b-b8f2-27505a395c75","metadata":{"executionCancelledAt":null,"executionTime":4966,"lastExecutedAt":1704817328092,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.memory import ConversationSummaryBufferMemory\n\nmemory = ConversationSummaryBufferMemory(llm=chatgpt, max_token_limit=100)\n\nmemory.save_context({\"input\":  \"Can you recommend me where should I travel next?\"}, \n                    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"})\n\nmemory.save_context({\"input\":  \"I love going to Museums\"}, \n                    {\"output\": \"Great then you should go to a cultural capital.\"})\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nconversation = ConversationChain(\n    llm=chatgpt, \n    memory = memory,\n    verbose=True\n)\n\nconversation.run(\"What cities do you recommend me?\")","outputsMetadata":{"0":{"height":353,"type":"stream"},"1":{"height":337,"type":"stream"},"2":{"height":97,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","I love going to Museums\n","================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," and learning about history and different cultures. It's fascinating to see how people lived in the past and how it has shaped our world today. Plus, museums often have interactive exhibits and activities that make learning even more fun. It's a great way to spend a day and expand my knowledge.\n","================================\u001b[1m Human Message \u001b[0m=================================\n","\n","Can you recommend me where should I travel next?\n","================================\u001b[1m Human Message \u001b[0m=================================\n","\n","\n","AI: That depends on your interests and budget. Some popular destinations for travelers include Paris, Rome, Tokyo, and New York City. If you enjoy nature and outdoor activities, you may want to consider visiting national parks or going on a safari in Africa. If you're interested in history and culture, places like Athens, Istanbul, and Cairo could be great options. Ultimately, it's important to research and plan a trip that aligns with your interests and budget.\n"]}],"source":["import uuid\n","from IPython.display import Image, display\n","from langchain_core.messages import HumanMessage\n","from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","# Define a new graph\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","# Define the function that calls the model\n","\n","\n","def call_model(state: MessagesState):\n","    response = model.invoke(state[\"messages\"])\n","    return {\"messages\": response}\n","\n","\n","# Define the two nodes we will cycle between\n","workflow.add_edge(START, \"model\")\n","workflow.add_node(\"model\", call_model)\n","\n","# Adding memory is straight forward in langgraph!\n","memory = MemorySaver()\n","\n","app = workflow.compile(\n","    checkpointer=memory\n",")\n","\n","# The thread id is a unique key that identifies\n","# this particular conversation.\n","# We'll just generate a random uuid here.\n","# This enables a single application to manage conversations among multiple users.\n","thread_id = uuid.uuid4()\n","config = {\"configurable\": {\"thread_id\": thread_id}}\n","\n","input_message = HumanMessage(content=\"I love going to Museums\")\n","for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n","    event[\"messages\"][-1].pretty_print()\n","\n","# Here, let's confirm that the AI remembers our previous messsage!\n","input_message = HumanMessage(\n","    content=\"Can you recommend me where should I travel next?\")\n","for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n","    event[\"messages\"][-1].pretty_print()"]},{"cell_type":"markdown","id":"e48c27ca-2d99-45d1-9d64-3436d095765e","metadata":{},"source":["### Dealing with Documents\n","\n","We are here to deal with documents... so LangChain provides a wide variety of elements to deal with them. \n","\n","One of the most important improvements of LangChain is that it allows us to upload documents and pass them to our model. \n","We consider a document as an object that holds a piece of text and metadata (more information about that text)\n","\n","- Document class\n","- Document Loader\n","- Document Retriever\n","- Text Splitter\n","- Index"]},{"cell_type":"markdown","id":"bd360d81-3068-44b3-b72b-451623776022","metadata":{},"source":["**TASK**\n","\n","1. From langchain.schema import the `Document` class. \n","2. Now define a document that has \n","   - Text contained in page_content. \n","   - Metada composed of document_id, document_source and document_create_time. "]},{"cell_type":"code","execution_count":8,"id":"52d33238-ce16-45fd-b216-1b9166341193","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1704817584415,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the document class\nfrom langchain.schema import Document\n\n# 2. Define the document:\nDocument(\n         page_content=\"This is a dummy document\",\n         metadata={\n             'document_id' : 677,\n             'document_source' : \"mysource.pdf\",\n             'document_create_time' : \"01/06/2022\"\n                   })"},"outputs":[{"data":{"text/plain":["Document(metadata={'document_id': 0, 'document_source': 'my_source', 'document_create_time': '01/01/2000'}, page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Import the document class\n","from langchain_core.documents import Document\n","\n","# 2. Define the document:\n","Document(page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\",\n","         metadata={\n","             'document_id': 0000,\n","             'document_source': \"my_source\",\n","             'document_create_time': \"01/01/2000\"\n","         })"]},{"cell_type":"markdown","id":"8c33e2ea-572e-4fbd-a348-5bcf0c961a8b","metadata":{},"source":["#### Document Loaders\n","\n","Depending on where our data is stored, we will need a different type of loader:\n","\n","- The **Online Loader** is used for loading a document directly from the Internet. LangChain implements different types of loaders. For example, there is the `WikipediaLoader` that helps you loading Wikipedia pages or the `HNLoader` to take content directly from any HackerNews page.\n","\n","\n","\n","- The **Offline Loader** is used loading a document stored that are already installed in your machine. There are also different types of offline loaders such as the **HTML** loader for `.html` pages or the **PyPDFLoader** for `.pdf` documents.\n","\n","In this project, we will see an example of Online Loader by using the `WikipediaLoader` and the `HNLoader`, and an example of Offline Loader by using the PyPDFLoader.\n","\n","You can find a list of the supported [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders) in the official documentation. Those Loaders are from external integrations, [native LangChain Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be found in the official documentation as well."]},{"cell_type":"code","execution_count":12,"id":"2599ff89-9838-4341-91a9-424eb3c4a49d","metadata":{"executionCancelledAt":null,"executionTime":5701,"lastExecutedAt":1704817696213,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.document_loaders import WikipediaLoader\n \n# Load content from Wikipedia using WikipediaLoader\nloader = WikipediaLoader(\"Machine_learning\")\nwikipedia_data = loader.load() #It returns a list of documents\n\nwikipedia_data[0]"},"outputs":[{"data":{"text/plain":["'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many field'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_community.retrievers import WikipediaRetriever\n","\n","# Load content from Wikipedia using WikipediaLoader\n","retriever = WikipediaRetriever()\n","# It returns a list of documents\n","wikipedia_data = retriever.invoke(\"Machine Learning\")\n","\n","wikipedia_data[0].page_content[:400]"]},{"cell_type":"code","execution_count":13,"id":"d3dca600-e561-4654-b688-7cec585ae80d","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1704817744183,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"\\nPage Content: \\n\", wikipedia_data[0].page_content)\nprint(\"\\nMeta Data: \\n\", wikipedia_data[0].metadata)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Page Content:  Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\n","ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\n","Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \n","From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n","\n","\n","== History ==\n","\n","The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\n","Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.\n","By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\n","Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\n","Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetic\n","\n","Meta Data:  {'title': 'Machine learning', 'summary': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.', 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}\n"]}],"source":["print(\"\\nPage Content: \", wikipedia_data[0].page_content)\n","print(\"\\nMeta Data: \", wikipedia_data[0].metadata)"]},{"cell_type":"markdown","id":"a6c5bc9d-5c18-4bb6-b30b-025da04e0862","metadata":{},"source":["**TASK:**\n","\n","Repeat the previous procedure using `HNLoader` and `PyPDFLoader`. \n","1. Import the corresponding Loader from langchain.document_loaders. \n","2. Initialize the loader indicating the source of data. \n","    - HNLoader -> https://news.ycombinator.com/item?id=34422627\n","    - PDFLoader -> Docs/attentions.pdf\n","    - PDFDirectoryLoader -> Docs/"]},{"cell_type":"code","execution_count":24,"id":"3a7b588e-8146-4df2-8e85-2e7d6d89cbb0","metadata":{"executionCancelledAt":null,"executionTime":2520,"lastExecutedAt":1704817928107,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Online Loader\nfrom langchain.document_loaders import HNLoader\nloader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\nhn_data = loader.load()\n\n# Load content from local PDFs\nfrom langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_data = loader.load()\n\n#We can use a directory loader to load more than one PDF at once. \n#loader = PyPDFDirectoryLoader(\"Docs/\")\n#pdf_directory_data = loader.load()"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Ozzie_osman on Jan 18, 2023  \\n             | next [–] \\n\\nLangChain is awesome. For people not sure what it\\'s doing, large language models (LLMs) are very powerful but they\\'re very general. As a common example for this limitation, imagine you want your LLM to answer questions over a large corpus.You can\\'t pass the entire corpus into the prompt. So you might:\\n- preprocess the corpus by iterating over documents, splitting them into chunks, and summarizing them\\n- embed those chunks/summaries in some vector space\\n- when you get a question, search your vector space for similar chunks\\n- pass those chunks to the LLM in the prompt, along with your questionThis ends up being a very common pattern, where you need to do some preprocessing of some information, some real-time collecting of pieces, and then an interaction with the LLM (in some cases, you might go back and forth with the LLM). For instance, code and semantic search follows a similar pattern (preprocess -> embed -> nearest-neighbors at query time -> LLM).Langchain provides a great abstraction for composing these pieces. IMO, this sort of \"prompt plumbing\" is far more important than all the slick (but somewhat gimicky) \"prompt engineering\" examples we see.I suspect this will get more important as the LLMs become more powerful and more integrated, requiring more data to be provided at prompt time.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Ozzie_osman on Jan 18, 2023  \\n             | parent | next [–] \\n\\nAlso, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_index) which takes a more \"data structure\" approach (and actually uses langchain for some stuff under the hood).'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"fireant on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nThank you for the link. I have a project with plenty of structured data from which I create summaries and short articles. Currently I'm generating prompts with context inside the prompt. GPT Index seems to pretty well fitted to that usecase\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bluecoconut on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis is great! I love seeing how rapidly in the past 6 months these ideas are evolving. I\\'ve been internally calling these systems \"prompt machines\". I\\'m a strong believer that chaining together language model prompts is core to extracting real, and reproducible value from language models. I sometimes even wonder if systems like this are the path to AGI as well, and spent a full month \\'stuck\\' on that hypothesis in October.Specific to prompt-chaining: I\\'ve spent a lot of time ideating about where \"prompts live\" (are they best as API endpoint, as cli programs, as machines with internal state, treated as a single \\'assembly instruction\\' -- where do \"prompts\" live naturally) and eventually decided on them being the most synonymous with functions (and api endpoints via the RPC concept)mental model I\\'ve developed (sharing in case it resonates with anyone else)a \"chain\" is `a = \\'text\\'; b = p1(a); c = p2(b)` where p1 and p2 are LLM prompts.What comes next (in my opinion) is other programming constructs: loops, conditionals, variables (memory), etc. (I think LangChain represents some of these concepts as their \"areas\" -> chain (function chaining), agents (loops), memory (variables))To offer this code-style interface on top of LLMs, I made something similar to LangChain, but scoped what i made to only focus on the bare functional interface and the concept of a \"prompt function\", and leave the power of the \"execution flow\" up to the language interpreter itself (in this case python) so the user can make anything with it.https://github.com/approximatelabs/lambdapromptI\\'ve had so much fun recently just playing with prompt chaining in general, it feels like the \"new toy\" in the AI space (orders of magnitude more fun than dall-e or chat-gpt for me). (I built sketch (posted the other day on HN) based on lambdaprompt)My favorites have been things to test the inherent behaviors of language models using iterated prompts. I spent some time looking for \"fractal\" like behavior inside the functions, hoping that if I got the right starting point, an iterated function would avoid fixed points --> this has eluded me so far, so if anyone finds non-fixed points in LLMs, please let me know!I\\'m a believer that the \"next revolution\" in machine-written code and behavior from LLMs will come when someone can tame LLM prompting to self-write prompt chains themselves (whether that is on lambdaprompt, langchain, or something else!)All in all, I\\'m super hyped about LangChain, love the space they are in and the rapid attention they are getting~'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"gandalfgeek on Jan 18, 2023  \\n             | parent | next [–] \\n\\nLambdaPrompt looks really cool. Love the Pythonic expression as good ol' functions, which makes the code read very naturally.What applications were you envisioning when you built it?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bluecoconut on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIn terms of applications with it, I have made things like sketch: https://github.com/approximatelabs/sketchRaw prompt-structure ideas i\\'ve worked with:- Iterate on a prompt with another \"discriminator\" prompt, that determines the result is good / safe- Write N-trials of an answer, then use another prompt to select the best answer- When doing code writing (SQL or Pandas) write the output, then use a parser (eg. `ast` in python) to validate code is valid, if not, feed back into a prompt for fixes- Logical negation checks (check if X, and if ~X, give opposite answers, then it\\'s likely consistent, if it\\'s both \"affirmative\" (as the models tend to bias towards), then it\\'s definitely hallucinating)Other \\'product\\' ideas i\\'ve tried:- A chat style interface (I made a chat-bot last year, similar to chatGPT)- A \"google-this-for-me\" style chain, that checks google, summarizes multiple results, then synthesizes a final resultIdeas I\\'ve been sitting on, that I think would be fun to prototype:- An iterative \"large document\" editor: storing global intent, instructions, outline, and the raw text, and each iteration of the prompt works on the these objects to build a large document.- A \"research this topic for me\", similar to the above, but include the google searching, summarizing, and such- A code-repository \"AI agent\" that takes `Issues` and `Pull requests` as input, and writes and edits code for you, and by adding feedback in github, it uses that to modify the branch and act as a developer. (Code via github interface, rather than an IDE)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='clxy on Jan 19, 2023  \\n             | root | parent | next [–] \\n\\nThanks for sharing your ideas! With respect to the research this topic for me concept, I stumbled upon this repo:https://github.com/daveshap/LiteratureReviewBotIt does something similar, but uses the ArXiv dataset to search PDFs instead of the internet.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"trentearl on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nThis resonates with me. I've been slowly working on similar project, instead of python functions I'm doing api creation. I have a website that allows you to create templated apis for each prompt.The next step for me is the workflow composition part. Instead of a functional  model of python functions Im going to try to compose workflows with AWS step functions where each step function calls a particular one of the templated api endpoints.I'm excited to see your progress.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | prev | next [–] \\n\\nLangChain is very cool. Templates and composability are the way forward.I'm guessing that most folks haven't created composable prompts, so a few experiences on why this stuff is necessary. I've been building a language learning app and it makes heavy use of GPT-3 [1], which has made clear a number of things for me:1) Composability is fundamental to leveraging language models. You can't get language models to just generate things in one go. It's rather like a human. If you're writing a book, you start with an idea, then an outline, then a chapter outline, then writing paragraphs... Or in our case, generate a flashcard deck description, then vocab, then example sentences, etc.2) Externalized prompt templates are also important. Engineers need to be able interface with experts who can create custom prompts. E.g. in our case, I need experts who speaks to build prompts specific to other languages for a language learning app [2].3) Unit testing is critical. There is no linter for a prompt. I have made so many typos over the last year that broke things. OpenAI has released several new models over the last 2 years. Anthropic is coming out with a model. You need to have assurances your prompts work. I've actually had to start building a basic unit tester for our prompts because of this... [3] (please someone else do this so I don't have to)4) external data is the next step forward for large language models. E.g. in my case, someone may want to learn about the history/culture of a country and we may want to reference existing articles on it since LLMs are known to hallucinate. We need to be able to interface with the web and databases easily. I'm not convinced that LangChain is the write layer of abstraction for this. I suspect/hope that the next version of GPT will have some significant advances in this regard.1. https://squidgies.app2. https://github.com/squidgyai/squidgy-prompts - open source composable prompts for Squidgies3. https://github.com/squidgyai/squidgy-prompts/tree/main/tests - unit test in YAML\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='beepbooptheory on Jan 18, 2023  \\n             | parent | next [–] \\n\\nSo much effort and processing power going into narrowing down and coercing a vastly general thing, makes me wonder at what point it stops being worth it to use the LLM.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='crosen99 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIt’s a lot of work and it’s messy, but the multiplier on productivity is too large to ignore. At some point all this will get easier, but for now it’s do or die.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='beepbooptheory on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nAfter the dust settles, what is it really multiplying?  Especially accounting for the energy/gpus it takes.  At the very least, why not generate all the cards once, and edit as needed?I just feel like this entire concept verges on this: https://imgur.com/EiGL1Z0'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='tintor on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nUnit testing: How would you automatically verify LLM output to a static prompt? Outputs can vary with each invocation.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='dandiep on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nTemperature = 0 solves a lot of it, but not all. A list of all options solves a portion of the remaining. Not sure what to do about the rest - maybe ask the LLM?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='tennisfan118 on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nBuilding https://promptable.ai to solve testing!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nAwesome... but I can't tell from your website what exactly it is.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='xrd on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nFWIW, your signup form has a field that creeps beyond the borders.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='dang on Jan 18, 2023  \\n             | prev | next [–] \\n\\nRelated ongoing thread:GPT-3.5 and Wolfram Alpha via LangChain - https://news.ycombinator.com/item?id=34422122'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"cs702 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nVery cool.By far the most interesting aspect of this, for me, is that we're now seeing tools for building software infrastructure with layers of APIs that operate on -- gasp! -- natural language, which is notoriously prone to imprecision and ambiguity. And yet it works remarkably well. It's hard not to look at all this, mouth agape, in awe.Part of me wonders, though:Wouldn't it be better if we could compose LLMs by passing sequences of embeddings (e.g., in a standardized high-dimensional space), which are much richer representations of LLM input, internal, and output states?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='firasd on Jan 18, 2023  \\n             | parent | next [–] \\n\\nI think embeddings are actually low-\\'resolution\\' representations. Like GPT\\'s ability to parse and calculate the structure of the sentence \"Hello, how are you?\" is not represented in the embedding for the sentence. The embedding is a 1-dimensional vector and inside the model it interacts with other texts in 10,000+ dimensions'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='neuronexmachina on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nI wonder if there\\'d be any use for a \"ontological\" representation, somewhere in-between a natural-language string and its embedding in a particular LLM. Maybe something that balances human-readability, LLM-composability, lack of brittleness, insight into the local structure of the embedding, etc.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nI wonder too. I imagine the best we could do with present technology is to get back the generated text in the form of text tokens accompanied by their corresponding deep embeddings (last hidden states): `[(text_token, deep_emb), (text_token, deep_emb), ...]`. Those deep embeddings incorporate \"everything the model knows\" about each generated token of text.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='neuronexmachina on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nMaybe a mapping/representation for a \"medium embedding\" could be learned that strikes a balance between shallow and deep. I have no idea what a good objective-function would be, though.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nI mean deep embeddings (i.e., sequences of hidden states, the ones are computed by all those interactions) , not the shallow embeddings of token ids in the first layer of the model! Those deep embeddings are much richer representations.Imagine if you and others building apps had access to \"GPT3 deep sequence embeddings v1.0\" via an API.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='machiaweliczny on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIt’t that precisely embeddings API from OpenAI? It has all context so very useful for search'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nNot quite. My understanding is that OpenAI\\'s various embeddings APIs return only a single vector per document, instead of the sequence of hidden states corresponding to each predicted next token in the response generated by a GPT-type LLM.Imagine getting generated text from a GPT LLM that comes with a deep embedding of each generated token\\'s \"contextual meaning\":  [(text_token, deep_emb), (text_token, deep_emb), ...]\\n\\nallowing higher-level models and apps to use all the information in those rich representations as inputs.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='swyx on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\n> And yet it works remarkably well.by which measure are you making this claim? even a 95% reliability means you get 5% wrong. on top of that you have prompt injection attacks. this stuff is much less suitable the more you move away from demos to predictable business applications'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nWhoa, I didn\\'t say this is suitable for predictable business applications yet!What I did say is that I\\'m in awe at the fact that this stuff works as well as it does, given that natural language is so notoriously prone to imprecision and ambiguity. I mean, if you had told me six months ago that this would be working even \"95%\" of the time in demos, I would have said, no way.Basically, I agree with you that at present this becomes \"less suitable the more you move away from demos to predictable business applications\" :-)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nPerhaps, but language is the common denominator in a multi-model world. E.g., I pass the GPT output into other models which are fine tuned for that sub domain. You can do embedding to embedding conversion, but not sure it's worth the effort.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='cs702 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nImagine if OpenAI made GPT3\\'s final hidden states available via an API (\"GPT3 deep sequence embeddings v1.0\"), next to each generated text token: [(text_token, deep_emb), (text_token, deep_emb), ...]. You and anyone else could build apps on top. Those hidden states would incorporate much more, and much richer, information than the text. Higher-level models could be trained to act on such information!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='LASR on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis is highly useful.I am prototyping new features where I work, on top of GPT3. To get it beyond fancy demos and actually delivering customer utility, you need a LOT of work to build in robustness and correctness.Prompt chains is where I’ve landed at the moment. This library seems very relevant and timely.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"alooPotato on Jan 18, 2023  \\n             | parent | next [–] \\n\\nIs your usage of GPT3 user facing in realtime? When you implement prompt chains, doesn't the UX become really slow?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"LASR on Jan 20, 2023  \\n             | root | parent | next [–] \\n\\nIt works well if you continously provide progress to the user. In my specific use case, the data generated is progressively displayed to the user, and refined in many further chain steps. They see the evolution of this. So it feels like it's fast, but also that a bunch of work is happening.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='dandiep on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nYes.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='jsemrau on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\n>you need a LOT of work to build in robustness and correctness.While bias is the more politically charged problem, variance is the problem of the current iteration that needs to be solved.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"hooande on Jan 18, 2023  \\n             | prev | next [–] \\n\\nIsn't this a chat interface to various apis? I guess that's the point. But it doesn't seem like that is utilizing the power of large language models. Maybe I'm not understanding this.Using a Google search api + a calculator to answer a question is cool [0]. but... we could already do that?[0] https://langchain.readthedocs.io/en/latest/modules/agents/ex...\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bob29 on Jan 18, 2023  \\n             | parent | next [–] \\n\\n>but... we could already do that?\\nthat sums up so much.\\nthe emperor has no clothes. few see it. \\nits inferior to the modern purpose built tools.\\nwe act impressed a simple program can be created from a prompt, what has every programmer been doing already for the last 10 years with google search and stackoverflow?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='motoxpro on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nAnother comment from this thread seems pretty cool to me.\\nhttps://github.com/approximatelabs/sketch'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='yunwal on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nI think building regexes is a good example of where chatgpt/copilot is useful. It’s not that it’s particularly difficult or requires much understanding, just very time consuming compared to writing an English language description and walking through an example.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='bob29 on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nUseful compared to writing a regex in notepad yeah.Last time I was challenged by regex I easily found very fancy web page with so many nice features. Actual documentation, ability to select a specific regex engine (or implementation or whatever you call it) , real-time results on test data, highlighting that shows how the regex works, etc. \\nand that was years ago I’m sure there’s even better web apps now.I can’t imagine having a better experience asking AI chat than using a web app made for the purpose'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='yunwal on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nDo you have a link to the site?Being able give some examples and just state in plain English what you want the capture groups to be is pretty much my ideal regex experience (in other words, I don’t want to think about the semantics of regex ever).'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"sandkoan on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nThis might be what they're referring to: https://regex101.com/.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='yunwal on Jan 19, 2023  \\n             | root | parent | next [–] \\n\\nThat’s what I figured. Maybe it’s just me but for complicated use-cases this still takes me forever to get the right regex string, especially with capture groups.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='KennyFromIT on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nThey have a dedicated chat bot for their docs that might be able to help you find answers to your questions...https://chat.langchain.dev/'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='user- on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nMe\\n> can i use docker?AI\\n> Yes, you can use Docker with LangChain. For more information, please see the Docker Installation Guide in the LangChain documentation.Then it links to a 404 lol. I checked the docs and there is nothing about docker in them. I wonder why its incorrect here'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"andreigheorghe on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nmaybe it's trained on non-published documentation pages?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"sebastiennight on Jan 18, 2023  \\n             | root | parent | prev | next [–] \\n\\nI'm very curious as to how they are able to limit the chat bot's knowledge to their own documentation.It seems as if it should also have the entire knowledge of the LLM, but> Who was Thomas Jefferson?Outputs> Hmm, I'm not sure. I'm an AI assistant for the open source library LangChain. You can find more information about LangChain at https://langchain.readthedocs.io.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='rafaquintanilha on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nYou can actually train the model to have access only to a specific body of text, like a documentation.Very useful article explaining this approach: https://dagster.io/blog/chatgpt-langchain'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='sebastiennight on Jan 20, 2023  \\n             | root | parent | next [–] \\n\\nWow that was a fascinating read! Thank you.It doesn\\'t explain why the model only refers to the documentation though.\\nBasically what they are doing is giving GPT-3 a prompt that includes the (semantically relevant) pieces of the documentation.But I don\\'t see why a User can\\'t ask about something that is tangentially relevant (\"Who is Bill Gates?\") and get an answer that really comes from GPT-3 pre-existing knowledge.And very clever way for these guys to go from \"hey we found this cool problem\" to \"well, did you notice that it ends up super complex and slow? Well well well, we could make it so much better with... wait for it... a data pipeline!\"(\"...and don\\'t we just happen to sell data pipeline software! What a coincidence!\")lol.\\nGreat read, thank you for the recommendation.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"mahastore on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nYes also the part I don't understand is that how langchain can be used to make the agents smarter for a particular domain? I did not see any interface to provide feedback of feed data. Maybe I am missing something?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Ozzie_osman on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\nIt\\'s usually either providing data to the LLM, or doing back-and-forth with the LLM (usually a mix of both).For instance, if you want it to answer questions about your code-base, the model doesn\\'t know your code base. You can\\'t feed the entire code-base into a prompt. So, you\\'d use langchain to:\\n  - preprocess your code-base, by chunking it and embedding it in some vector space\\n  - when you get a question, see where it is in the vector space and find the \"k nearest neighbors\"\\n  - pass those nearest neighbors, along with your question, to the LLM (because those neighbors are the contextually relevant pieces, and they\\'d fit in the prompt)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='delijati on Jan 18, 2023  \\n             | root | parent | next [–] \\n\\ni would really love to give it a try but with an offline LLM :/'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='peterth3 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nWhat LLMs does LangChain support?Btw I asked chat.langchain.dev and it said:> LangChain uses pre-trained models from Hugging Face, such as BERT, GPT-2, and XLNet. For more information, please see the Getting Started Documentation[0].That links to a 404, but I did find the correct link[1]. Oddly that doc only mentions an OpenAI API wrapper. I couldn’t find anything about the other models from huggingface.Does LangChain have any tooling around fine tuning pre-trained LLMs like GPTNeoX[2]?[0]https://langchain.readthedocs.io/en/latest/getting_started.h...[1]https://langchain.readthedocs.io/en/latest/getting_started/g...[2]https://github.com/EleutherAI/gpt-neox'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='lgas on Jan 18, 2023  \\n             | parent | next [–] \\n\\nHere are the docs on the built in models at the moment: https://langchain.readthedocs.io/en/latest/reference/modules...One of their examples is    from langchain import NLPCloud\\n    nlpcloud = NLPCloud(model=\"gpt-neox-20b\")\\n\\nSo it looks like you\\'re good to go.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='throwaway20222 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis is amazing. Thank you to all the contributors!I am working on a project in the old media space and we were planning to build several of these elements ourselves if something like this didn’t come along. Love the open source nature and would love to contribute.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='brotchie on Jan 18, 2023  \\n             | prev | next [–] \\n\\nHell yeah. Been thinking about this in my spare cycles a lot! The missing thing when interacting with GPT3 was building up layers of re-usable abstraction that could be mixed together: i.e. a prompt that\\'s great at summarization, a prompt that great at generating a list of 5 x related ideas, a prompt that\\'s great at sorting a list of inputs according to some verbal sorting description.Had a play around with this early-GPT3 days by creating a Python decorator that let you easily write \"prompt functions\" and then you could combine these to create higher-level prompt generating machines.LangChain takes this to the logical end-point, awesome!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='owlninja on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI like to think I am smart and in the loop - but would this allow me to feed all sorts of PowerPoint presentations, word docs, text files, etc... so that new hires could get a concise answer to business processes?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"dandiep on Jan 18, 2023  \\n             | parent | next [–] \\n\\nThere's a lot of work going on in this area right now. Check out GPT Index:https://github.com/jerryjliu/gpt_index\\nhttps://github.com/jerryjliu/gpt_index/blob/main/examples/pa...GPT isn't multi-modal yet (so no images), but that's coming.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='camjw on Jan 18, 2023  \\n             | prev | next [–] \\n\\nWe’re using LangChain at my startup to orchestrate all the GPT-3 calls and it’s fantastic! Harrison is so helpful too!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='monkeydust on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI have a side project at the moment to use LangChain to help build a Q&A system from domain specific documentation on a product I own. Its around 500 different PDFs. I was going to follow this guide* which looks credible and solves for how to deal with large corpus of texts. Curious what others think as to approach.* https://dagster.io/blog/chatgpt-langchain'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"revskill on Jan 18, 2023  \\n             | prev | next [–] \\n\\nIt's sad that all things now has OpenAI dependency. That OpenAI is political, it's banned in some regions in the world.Academics became a shameful thing.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='fudged71 on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI’ve been following both LangChain and GPT Index but to be honest I’m getting super confused which use cases are better suited for each (or a combination of both!). It would be great to see a presentation of both tools together like this :)'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='Mizza on Jan 18, 2023  \\n             | prev | next [–] \\n\\nI had a similar idea, but it uses a graph construction and Elixir. Definitely going to poach some ideas from LangChain though.https://github.com/Miserlou/Helix'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='imranq on Jan 18, 2023  \\n             | prev | next [–] \\n\\nIs there support for Prompt-Tuning? This is also an interesting space to optimize inputs to frozen language modelshttps://ai.googleblog.com/2022/02/guiding-frozen-language-mo...'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"mfalcon on Jan 18, 2023  \\n             | prev | next [–] \\n\\nWhat if OpenAI decides to close or charge for access to it's API? Is there a compromise of keeping it open somewhere? What's the plan B?There's a really strong dependency on their service and I sometimes get doubtful about dedicating time to build things on top of it.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"ronsor on Jan 18, 2023  \\n             | parent | next [–] \\n\\n> OpenAI decides to close or charge for access to it's API?They already do: https://openai.com/api/pricing/\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='mbil on Jan 18, 2023  \\n             | parent | prev | next [–] \\n\\nOpenAI already does charge access to its API. ChatGPT is currently free, but LLM model access has been available through a paid API since before GPT-3.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"Animats on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThis looks useful. It should compete with Rasa, which is supposed to do something similar, but in practice is mostly only able to match what you're saying to nodes in a phone tree. Has anyone done a game NPC with this?\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='barrenko on Jan 18, 2023  \\n             | prev | next [–] \\n\\nPretty sweet, but are there similar setup more geared towards coding?This would be a great way to get a pair programmer for people who are just coding for hobby or transitioning to SWE. Or to alleviate imposter syndrome.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='swyx on Jan 18, 2023  \\n             | parent | next [–] \\n\\ncopilot not enough for you? why?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"mark_l_watson on Jan 18, 2023  \\n             | prev | next [–] \\n\\nSuch a good idea! A few years ago I tried importing pre-trained Keras models into Racket Scheme, creating a function for each model. I didn't really get the idea right, LangChain looks much better.\"), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='ghoshbishakh on Jan 18, 2023  \\n             | prev | next [–] \\n\\nThank god this is not about Blockchains.'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='jarbus on Jan 18, 2023  \\n             | prev | next [–] \\n\\nAhhh, I was hoping to work on something like this after I completed my PhD!'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='alonger1999 on Jan 23, 2023  \\n             | prev | next [–] \\n\\nHi'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content='keepquestioning on Jan 18, 2023  \\n             | prev [–] \\n\\ngamechangerhow can we use ChatGPT to design a 3d model?'), Document(metadata={'source': 'https://news.ycombinator.com/item?id=34422627', 'title': 'LangChain: Build AI apps with LLMs through composability'}, page_content=\"lgas on Jan 18, 2023  \\n             | parent [–] \\n\\nThis doesn't really seem like the right thread for this question, but as long as we are here... you might be interested in POINT-E https://huggingface.co/spaces/openai/point-e or Stable Dreamfusion https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUU...\")]\n","[Document(metadata={'source': 'Docs/attentions.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]\n"]}],"source":["# Online Loader\n","from langchain_community.document_loaders.pdf import PyPDFLoader\n","from langchain_community.document_loaders.hn import HNLoader\n","\n","# Load content from website\n","loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n","hn_data = loader.load()\n","print(hn_data)\n","\n","# Load content from local PDFs\n","loader = PyPDFLoader(\"Docs/attentions.pdf\")\n","pdf_data = loader.load()\n","print(pdf_data)\n","\n","# We can use a directory loader to load more than one PDF at once.\n","# loader = PyPDFDirectoryLoader(\"./\")\n","# pdf_directory_data = loader.load()"]},{"cell_type":"markdown","id":"da29979d-f779-41e1-a35c-598770a03139","metadata":{},"source":["#### Text Splitter\n","\n","**Data Chunks and Model Tokenizer**\n","\n","To efficiently handle data when building an LLM-based application, data needs to be divided in portions. Those are the so-called data chunks and the chunk size is highly determinant in the quality of the chatbot.\n","\n","The tokenizer plays a crucial role in relation to data chunks when working with LLMs: \n","- A **tokenizer is the tool used to convert text data into a format that can be processed by the model.**\n","- Data is then stored in the vector stores in the tokenized format.\n","\n","To convert the original data into tokens and split it in data chunks, we will use the **LangChain Text Splitter**.\n","\n","If you are interested in more details about the tokenizer, the article [Unleashing the ChatGPT Tokenizer](https://medium.com/towards-data-science/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54) is for you!\n"]},{"cell_type":"markdown","id":"c72a417e-7708-4d1e-bc99-0b0ba703dbe1","metadata":{},"source":["By using Langchain, we can highly customize how to split our data:\n","- **Split by chunks**: The most general approach is to split your data into chunks of a concrete size. In the following example, we will take the data that we have already loaded (`wikipedia_data`, `hn_data` and `pdf_data`) and we will split it in portions of 200 characters. \n","\n","_What will happen if the split based on character count breaks a word?_\n","\n","There is the concept of \"chunk overlap\" that refers to a method where consecutive chunks of text share some common content. This technique is used to maintain context and coherence when a long document is divided into smaller parts due to the token limitations of LLMs. In this case, we will use a chunk size of 20 characters.\n","\n","So let's split the Wikipedia data we have just loaded: "]},{"cell_type":"code","execution_count":18,"id":"b5803ebc","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --quiet langchain-text-splitters"]},{"cell_type":"code","execution_count":25,"id":"4df0ebc1-87a3-4755-a05d-4088db03b29d","metadata":{"executionCancelledAt":null,"executionTime":403,"lastExecutedAt":1704818041060,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Advanced method - Split by chunks ________________________________________________________________________\n# Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\nprint(\"\\nSPLITTING BY CHUNKS\")\nwikipedia_chunks = text_splitter.split_documents(wikipedia_data)\nprint(\"Wikipedia Data - Now you have {0} number of chunks.\".format(len(wikipedia_chunks)))","outputsMetadata":{"0":{"height":80,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":77,"type":"stream"},"4":{"height":77,"type":"stream"},"5":{"height":57,"type":"stream"},"6":{"height":77,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","SPLITTING BY CHUNKS\n","Wikipedia Data - Now you have 17 number of chunks.\n"]}],"source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from transformers import GPT2TokenizerFast\n","\n","# Advanced method - Split by chunks\n","# Create function to count tokens\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","\n","def count_tokens(text: str) -> int:\n","    return len(tokenizer.encode(text))\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=200,\n","    chunk_overlap=20,\n","    length_function=count_tokens  # It uses len() by default.\n",")\n","\n","print(\"\\nSPLITTING BY CHUNKS\")\n","wikipedia_chunks = text_splitter.split_documents(wikipedia_data)\n","print(\n","    \"Wikipedia Data - Now you have {0} number of chunks.\".format(len(wikipedia_chunks)))"]},{"cell_type":"markdown","id":"2cb800f3-0b01-4bff-9291-1736cb5cdbe4","metadata":{},"source":["**TASK:**\n","\n","Generate the chunks for both `HNLoader` and `PyPDFLoader`. \n","1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n","2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","3. Define a count_tokens function that will allow us to count the tokens of out text. \n","4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n","5. Apply the command `.split_documents`to our data. \n","\n","You can define your own function for the HN data and use your the default function for the PDF Data.  "]},{"cell_type":"code","execution_count":26,"id":"bd61a742-dc5e-441a-a746-37d43012aceb","metadata":{"executionCancelledAt":null,"executionTime":288,"lastExecutedAt":1704818288170,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#_____________________________________________________________________PDF DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n \n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=20,\n    length_function=count_tokens\n)\n\n# 5 - Apply the .split_document command\npdf_chunks = text_splitter.split_documents(pdf_data)\nprint(\"PDF Data - Now you have {0} number of chunks.\".format(len(pdf_chunks)))\n\n\n#_____________________________________________________________________HN DATA\n# 3 - We use the default len, no need to do anything.\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=200,\n    chunk_overlap=20\n)\n\nhn_chunks = text_splitter.split_documents(hn_data)\nprint(\"Online HN - Now you have {0} number of chunks.\".format(len(hn_chunks)))","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["HN Data - Now you have 15 number of chunks.\n","Local PDF - Now you have 90 number of chunks.\n"]}],"source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from transformers import GPT2TokenizerFast\n","\n","# Apply the .split_document command for PDF data\n","pdf_chunks = text_splitter.split_documents(pdf_data)\n","print(\"HN Data - Now you have {0} number of chunks.\".format(len(pdf_data)))\n","\n","# Apply the .split_document command for HN DATA\n","hn_chunks = text_splitter.split_documents(hn_data)\n","print(\"Local PDF - Now you have {0} number of chunks.\".format(len(hn_chunks)))"]},{"cell_type":"markdown","id":"ad447dd1-d5a5-4b2e-91ce-ef41539ddf73","metadata":{},"source":["We can make sure that the chunking has been successful by visualizing the distribution of chunk sizes. \n","Since we have selected a chunk size of 200, the majority of our chunks should have this lenght:"]},{"cell_type":"code","execution_count":27,"id":"31777cc6-9a05-45d6-b1b6-01db6587270c","metadata":{"executionCancelledAt":null,"executionTime":309,"lastExecutedAt":1704818302589,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Quick data visualization to ensure chunking was successful\n\n# Create a list of token counts\ntoken_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n\n# Create a DataFrame from the token counts\ndf = pd.DataFrame({'Token Count': token_counts})\n\n# Create a histogram of the token count distribution\ndf.hist(bins=40, )\n\n# Show the plot\nplt.show()"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Quick data visualization to ensure chunking was successful\n","# Create a list of token counts\n","token_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n","\n","# Create a DataFrame from the token counts\n","df = pd.DataFrame({'Token Count': token_counts})\n","\n","# Create a histogram of the token count distribution\n","df.hist(bins=40, )\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","id":"c149bde7-39f5-4912-83f2-fb238e4d72a7","metadata":{},"source":["- **Split by pages**: If your data comes from documents organized in pages, there are methods that allow you to split data in pages to keep track of the page content. This method is specially useful when dealing with PDFs, as in the following example:"]},{"cell_type":"code","execution_count":23,"id":"87a8e84c-d658-4a7a-8144-d84fcb169928","metadata":{"executionCancelledAt":1704727738639,"executionTime":1063,"lastExecutedAt":1704708532500,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Simple method - Split by pages    ________________________________________________________________________\n# You need a PDF file in your environement. \nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_pages_chunks = loader.load_and_split()\npdf_pages_chunks\n\nprint(\"\\nSPLITTING BY PAGES\")\nprint(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))","outputsMetadata":{"0":{"height":77,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":137,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","SPLITTING BY PAGES\n","PDF Splited by Pages - You have 16 number of chunks.\n"]}],"source":["# Simple method - Split by pages    ________________________________________________________________________\n","# You need a PDF file in your environement.\n","loader = PyPDFLoader(\"Docs/attentions.pdf\")\n","pdf_pages_chunks = loader.load_and_split()\n","pdf_pages_chunks\n","\n","print(\"\\nSPLITTING BY PAGES\")\n","print(\n","    \"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))"]},{"cell_type":"markdown","id":"06933573-24eb-4ba2-bf05-4410912be27e","metadata":{},"source":["### Vector Stores\n","\n","Vector stores, also known as vector databases, are specialized types of databases designed to efficiently handle and manipulate high-dimensional vector data. In our case, we will store the tokenized and splitted content, e.g., the data chunks in the format that LLMs can process.\n","\n","There are different types of vector stores. Depending on the storage of the data, we can classify them as:\n","- **Local Vector Stores**: This type of databases store the information in your local system. As an example of Local Vector Store, we will use FAISS.\n","- **Online Vector Stores**: This type of databases store the information in the cloud. We will use Pinecone as out preferred option for Online Vector Stores.\n","\n","FAISS - EXAMPLE OF LOCAL VECTOR STORE"]},{"cell_type":"code","execution_count":28,"id":"b7dc17d0-275f-4376-86e1-11661a2dd1ef","metadata":{"executionCancelledAt":null,"executionTime":4673,"lastExecutedAt":1704818404491,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.vectorstores import FAISS  # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Get embedding model\nembeddings = OpenAIEmbeddings()\n\n# OPTION 1: FAISS (Facebook AI Similarity Search) Local _______________________________________________________________________________________\n# Create vector database\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)","outputsMetadata":{"0":{"height":580,"type":"stream"},"1":{"height":616,"type":"stream"}}},"outputs":[],"source":["# Get embedding model\n","embeddings = OpenAIEmbeddings()\n","\n","# OPTION 1: FAISS (Facebook AI Similarity Search) Local\n","# Create vector database\n","db_FAISS = FAISS.from_documents(pdf_chunks, embeddings)"]},{"cell_type":"markdown","id":"1c4faca6-6ae9-453c-90d0-b31fc75a7872","metadata":{},"source":["PINECONE - EXAMPLE OF ONLINE VECTOR STORE"]},{"cell_type":"code","execution_count":29,"id":"5bb277e5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --quiet pinecone\n","%pip install --quiet langchain-pinecone"]},{"cell_type":"code","execution_count":52,"id":"776ae3bd-a6d2-4066-881a-dd08cc9f9ede","metadata":{"executionCancelledAt":null,"executionTime":2496,"lastExecutedAt":1704818462367,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pinecone  #We need the Pinecone library to initialize our connection.\nfrom langchain.vectorstores import Pinecone # for the vector database part -- Pinecone is cloud-\n# OPTION 2: PINECONE Online\n     \n# We initialize pinecone\npinecone.init(      \n\tapi_key=os.getenv(\"PINECONE_API_KEY\"),      \n\tenvironment=os.getenv(\"PINECONE_ENV_KEY\")     \n) \n\n# Create a new pinecone index\n#pinecone.create(name=\"langchain\", dimension=1536, metric=\"cosine\")\n\n# We define the name of our index (in case the index is already created)\nindex_name = \"langchain\"\n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\ndb_Pinecone = Pinecone.from_documents(pdf_chunks, embeddings, index_name=index_name)"},"outputs":[],"source":["from pinecone import Pinecone, ServerlessSpec\n","from langchain_pinecone import PineconeVectorStore\n","\n","# We initialize pinecone\n","pinecone = Pinecone(\n","    api_key=os.getenv(\"PINECONE_API_KEY\")\n",")\n","\n","existing_indexes = [index_info[\"name\"]\n","                    for index_info in pinecone.list_indexes()]\n","\n","# We define the name of our index (in case the index is already created)\n","index_name = \"langchain\"\n","\n","if index_name not in existing_indexes:\n","    pinecone.create_index(\n","        name=index_name,\n","        dimension=1536,\n","        metric=\"cosine\",\n","        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n","    )\n","    while not pinecone.describe_index(index_name).status[\"ready\"]:\n","        time.sleep(1)\n","\n","\n","# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\n","db_Pinecone = PineconeVectorStore.from_documents(\n","    pdf_chunks, embeddings, index_name=index_name)"]},{"cell_type":"markdown","id":"e2e6151d-08d3-4c8f-8a7a-643c101bd701","metadata":{},"source":["### Natural Language Retrieval\n","We first start performing a semantic search within our Vector DataBase. "]},{"cell_type":"code","execution_count":66,"id":"826da9fb-03ba-4101-b793-3b9ac3203d79","metadata":{"executionCancelledAt":null,"executionTime":900,"lastExecutedAt":1704818532774,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=2)\nprint(matches)","outputsMetadata":{"0":{"height":437,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(metadata={'source': 'Docs/attentions.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 13}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14')]\n"]}],"source":["# We can define how many similarities we want to get back by defining the variable k\n","query = \"Can you please tell me all the autors of the article Attention is all you need?\"\n","matches = db_FAISS.similarity_search(query, k=2)\n","print(matches)"]},{"cell_type":"markdown","id":"17489dca-2a53-40ab-9b12-13a65e1f8c53","metadata":{},"source":["In the above section, we have seen how to retrieve the coincidences of you query in the documents in our vector store. Nevertheless, the output is a bit difficult to read. We can leverage the usage of LLMs by feeding the coincidences in our vector store to an LLM and let it generate a response in Natural Language using the additional information from our documents. We can do so by using the so-called **[LangChain Chains](https://python.langchain.com/docs/expression_language/get_started)**."]},{"cell_type":"code","execution_count":67,"id":"ccc9684f-f666-473f-8a48-9c8f53e413ab","metadata":{"executionCancelledAt":null,"executionTime":2981,"lastExecutedAt":1704818563496,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# ONLINE - PINECONE\n\n# 1. Define our query of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 2. Perform the semantic search in our vector database with the similarity_search command.  \nmatches = db_Pinecone.similarity_search(query, k=2)\n\n# 3. Define a load_qa_chain.\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Execute the chain with the prompt and the matches. \nchain.run(input_documents=matches, question = query)","outputsMetadata":{"0":{"height":257,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input': \"Can you please tell me all the authors of the article 'Attention is All You Need'?\", 'context': [Document(metadata={'source': 'Docs/attentions.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 13}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive'), Document(metadata={'source': 'Docs/attentions.pdf', 'page': 14}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')], 'answer': \"\\n\\nSystem: The authors of the article 'Attention is All You Need' are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\"}\n"]}],"source":["from langchain.chains import create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","# Prepare the vector store to be used by the model\n","pdf_retriever = db_FAISS.as_retriever()\n","\n","# Define the query and the system prompt\n","query = \"Can you please tell me all the authors of the article 'Attention is All You Need'?\"\n","system_setup = (\n","    \"You are a PDF manager that helps to understand a document that's specified \"\n","    \"and offers information on it. Use the following pieces of context to answer the question.\"\n","    \"\\n\\n\"\n","    \"{context}\"  # Ensure that the prompt has a placeholder for the context.\n",")\n","\n","# Create the prompt template\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system_setup),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","# Create the question-answering chain and the retrieval chain\n","question_answer_chain = create_stuff_documents_chain(model, prompt)\n","rag_chain = create_retrieval_chain(pdf_retriever, question_answer_chain)\n","\n","# Run the chain with the input query\n","result = rag_chain.invoke({\"input\": query})"]},{"cell_type":"code","execution_count":68,"id":"614e51fe","metadata":{},"outputs":[{"data":{"text/plain":["{'input': \"Can you please tell me all the authors of the article 'Attention is All You Need'?\",\n"," 'context': [Document(metadata={'source': 'Docs/attentions.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡'),\n","  Document(metadata={'source': 'Docs/attentions.pdf', 'page': 13}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n","  Document(metadata={'source': 'Docs/attentions.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive'),\n","  Document(metadata={'source': 'Docs/attentions.pdf', 'page': 14}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')],\n"," 'answer': \"\\n\\nSystem: The authors of the article 'Attention is All You Need' are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\"}"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["result"]},{"cell_type":"code","execution_count":69,"id":"d164cac5-2244-4582-b567-e9ae4461afdd","metadata":{"executionCancelledAt":null,"executionTime":2611,"lastExecutedAt":1704818697699,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=4)\n\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\nchain.run(input_documents=matches, question = query)\n"},"outputs":[{"data":{"text/plain":["{'input': \"Can you please tell me all the authors of the article 'Attention is All You Need'?\",\n"," 'context': [Document(id='f1e6ef15-8d06-4848-995d-a1f930ccbc87', metadata={'page': 0.0, 'source': 'Docs/attentions.pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡'),\n","  Document(id='05ab80f5-28b6-4c86-87ce-3e1644472359', metadata={'page': 13.0, 'source': 'Docs/attentions.pdf'}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n","  Document(id='27c87597-be81-403d-8541-39c6fb8e3b6a', metadata={'page': 11.0, 'source': 'Docs/attentions.pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive'),\n","  Document(id='2cf6b6b3-dc42-41f5-8c18-559475336ba9', metadata={'page': 14.0, 'source': 'Docs/attentions.pdf'}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')],\n"," 'answer': \"\\n\\nSystem: The authors of the article 'Attention is All You Need' are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\"}"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["# PINECONE example\n","\n","# pinecone retrieval\n","pinecone_retrieval = db_Pinecone.as_retriever()\n","\n","# Create the question-answering chain and the retrieval chain\n","question_answer_chain = create_stuff_documents_chain(model, prompt)\n","rag_chain = create_retrieval_chain(pinecone_retrieval, question_answer_chain)\n","\n","# Run the chain with the input query\n","result = rag_chain.invoke({\"input\": query})\n","result"]},{"cell_type":"markdown","id":"d164cac5-2244-4582-b567-e9ae4461afdd","metadata":{"executionCancelledAt":null,"executionTime":2611,"lastExecutedAt":1704818697699,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=4)\n\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\nchain.run(input_documents=matches, question = query)\n"},"source":["### Indexes and Metadata\n","When we upload data to our vector database, there is metadata that allows us to understand where the data is coming from. \n","When dealing with PDFs, the source information allows us to know what pdf and page the info is coming from."]},{"cell_type":"code","execution_count":74,"id":"97d1db0b-b343-4836-b43b-8e4ab735c033","metadata":{"executionCancelledAt":null,"executionTime":174,"lastExecutedAt":1704818705089,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#from langchain.embeddings import OpenAIEmbeddings\n#from langchain.indexes import index\n\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\n\nprint(\"______________________________________ THIRD MATCH\")\n\nprint(\"We can get the chunk text content and get: \\n\", matches[3].page_content)\nprint(\"\\nWe can get the chunk metadata and get: \\n\", matches[3].metadata)\nprint(\"\\nThe source of our match is: \\n\" , matches[3].metadata[\"source\"], \"and page\", matches[3].metadata[\"page\"])","outputsMetadata":{"0":{"height":542,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["FIRST MATCH FROM PDF VECTOR DATABASE\n","We can get the chunk text content and get: \n"," To the best of our knowledge, however, the Transformer is the first transduction model relying\n","entirely on self-attention to compute representations of its input and output without using sequence-\n","aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n","self-attention and discuss its advantages over models such as [17, 18] and [9].\n","3 Model Architecture\n","Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n","Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n","of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n","sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n","\n","We can get the chunk metadata and get: \n"," {'source': 'Docs/attentions.pdf', 'page': 1}\n","\n","The source of our match is: \n"," Docs/attentions.pdf and page 1\n"]}],"source":["query = \"what are transformers?\"\n","matches = db_FAISS.similarity_search(query)\n","\n","print(\"FIRST MATCH FROM PDF VECTOR DATABASE\")\n","\n","print(\"We can get the chunk text content and get: \\n\", matches[0].page_content)\n","print(\"\\nWe can get the chunk metadata and get: \\n\", matches[0].metadata)\n","print(\"\\nThe source of our match is: \\n\",\n","      matches[3].metadata[\"source\"], \"and page\", matches[0].metadata[\"page\"])"]},{"cell_type":"markdown","id":"f2e5f032-6e4d-40b2-b851-ccb9c86fc097","metadata":{},"source":["Now it is the time to put it all together and generate a simple pipeline to query our documents using a LLM model. "]},{"cell_type":"markdown","id":"4d0282a5-73c9-44b8-87fd-eb175d676748","metadata":{},"source":["# PART 2: Loading and processing our documents\n","\n","\n","\n","PyPDFDirectoryLoader allows us to upload multiple PDFs at once. In our case, we have two PDFs in the Docs directory."]},{"cell_type":"markdown","id":"89db047f-2475-4738-a726-458a5799a21e","metadata":{},"source":["## **STEP 1 - LOADER**\n","\n","Use the `PDFDirectoryLoader` to upload all PDFs contained within the the Docs folder. "]},{"cell_type":"code","execution_count":null,"id":"a374a538-47c8-4679-a5d6-6c3e4a8a208c","metadata":{"executionCancelledAt":null,"executionTime":13294,"lastExecutedAt":1704818828591,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#STEP 1 - LOADER\nfrom langchain.document_loaders import PyPDFDirectoryLoader\n\nloader = PyPDFDirectoryLoader(\"Docs/\")\n\ndata = loader.load()"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"d8aebdc5-1686-4401-89d2-3b59546f8d3c","metadata":{},"source":["## **STEP 2 - CHUNKING**\n","\n","Generate the chunks for the PDFs contained in the directory. \n","1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n","2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n","3. Define a count_tokens function that will allow us to count the tokens of out text. \n","4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n","5. Apply the command `.split_documents`to our data. "]},{"cell_type":"code","execution_count":null,"id":"198312a1-bd33-4922-b5fd-1e402cd10c91","metadata":{"executionCancelledAt":null,"executionTime":1378,"lastExecutedAt":1704818978129,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#STEP 2 - CHUNKING OUR DATA\n#_____________________________________________________________________PDFs Data\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\nchunks = text_splitter.split_documents(data)\nprint(\"Multiple PDFs - Now you have {0} number of chunks.\".format(len(chunks)))","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":38,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["Multiple PDFs - Now you have 610 number of chunks.\n"]}],"source":[]},{"cell_type":"markdown","id":"8f548885-aee9-4b4a-bf1d-e996ed02c74d","metadata":{},"source":["## **STEP 3 - EMBEDD AND UPLOAD THE DATA INTO A VECTORSTORE**\n","\n","**TASK**\n","- Upload the data into the FAISS vector store using the `from_documents`command. "]},{"cell_type":"code","execution_count":null,"id":"04a6a94c-153d-438a-978b-df189a33fa6c","metadata":{"executionCancelledAt":null,"executionTime":4211,"lastExecutedAt":1704819033112,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 3 - EMBEDDING AND UPLOAD DATA INTO OUR VECTORSTORE\n\n# ___________________________________________________________________________ LOCAL VERSION\n\n# 1. Create vector database with FAISS\ndb_FAISS = FAISS.from_documents(chunks, embeddings)\n\n# Check similarity search is working\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\nprint(\"We found {0} number of similarities.\".format(len(matches)))\nfor match in matches:\n    print(\"\\n\", match.page_content)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["We found 4 number of similarities.\n","\n"," To the best of our knowledge, however, the Transformer is the first transduction model relying\n","entirely on self-attention to compute representations of its input and output without using sequence-\n","aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n","self-attention and discuss its advantages over models such as [17, 18] and [9].\n","3 Model Architecture\n","Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n","Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\n","of continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\n","sequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n","\n"," vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior\n","works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al.,\n","2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but\n","these mostly focus on code retrieval, classiﬁcation, and program repair. Several recent and concurrent\n","efforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al.,\n","2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they\n","focus on generating code in a single turn, we propose to factorize the speciﬁcations into multiple turns\n","\n"," The Transformer allows for significantly more parallelization and can reach a new state of the art in\n","translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n","2 Background\n","The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n","[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n","block, computing hidden representations in parallel for all input and output positions. In these models,\n","the number of operations required to relate signals from two arbitrary input or output positions grows\n","in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n","it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n","reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n","\n"," illia.polosukhin@gmail.com\n","Abstract\n","The dominant sequence transduction models are based on complex recurrent or\n","convolutional neural networks that include an encoder and a decoder. The best\n","performing models also connect the encoder and decoder through an attention\n","mechanism. We propose a new simple network architecture, the Transformer,\n","based solely on attention mechanisms, dispensing with recurrence and convolutions\n","entirely. Experiments on two machine translation tasks show these models to\n","be superior in quality while being more parallelizable and requiring significantly\n","less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n","to-German translation task, improving over the existing best results, including\n","ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n"]}],"source":[]},{"cell_type":"markdown","id":"37afb3bf-6c25-4562-b9c7-f32d17729cf9","metadata":{},"source":["# PART 3: Talking with our documents"]},{"cell_type":"markdown","id":"b5d54ac1-c3ff-42c3-a9c8-151548914489","metadata":{},"source":["## STEP 4 - DEFINE A CHAIN AND PERFORM THE SIMILARITY SEARCH\n","Generating a simple pipeline to query our documents with a load_qa_chain. \n","**TASK**\n","1. Import the `load_qa_chain`from the langchain.chains.question_answering library. \n","2. Define a prompt of interest, like: \"Can you please tell me all the autors of the article Attention is all you need?\"\n","3. Define the chain.\n","4. Perform a semantic search with the `.similarity_search`. \n","5. Execute the chain. "]},{"cell_type":"code","execution_count":null,"id":"85627ed4-89b0-475c-b7ea-caba04fce893","metadata":{"executionCancelledAt":null,"executionTime":1844,"lastExecutedAt":1704819185918,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. Execute the chain to obtain a NLP based response. \nresponse = chain.run(input_documents = matches, question = query)\nprint(response)","outputsMetadata":{"0":{"height":227,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["The authors of the article \"Attention Is All You Need\" are:\n","\n","1. Ashish Vaswani\n","2. Noam Shazeer\n","3. Niki Parmar\n","4. Jakob Uszkoreit\n","5. Llion Jones\n","6. Aidan N. Gomez\n","7. Łukasz Kaiser\n","8. Illia Polosukhin\n"]}],"source":[]},{"cell_type":"markdown","id":"3570da7e-07ca-4dbd-a390-0f74aeabc044","metadata":{},"source":["Now that we already have a working pipeline to query our documents, we want to understand where our data is coming from. "]},{"cell_type":"code","execution_count":null,"id":"c9445c4e-6c17-4a2f-8a5e-0456873c8ac6","metadata":{"executionCancelledAt":null,"executionTime":3750,"lastExecutedAt":1704819277172,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","outputsMetadata":{"0":{"height":269,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["The authors of the article \"Attention Is All You Need\" are:\n","\n","1. Ashish Vaswani - Google Brain (source: attentions.pdf, page 0)\n","2. Noam Shazeer - Google Brain (source: attentions.pdf, page 0)\n","3. Niki Parmar - Google Research (source: attentions.pdf, page 0)\n","4. Jakob Uszkoreit - Google Research (source: attentions.pdf, page 0)\n","5. Llion Jones - Google Research (source: attentions.pdf, page 0)\n","6. Aidan N. Gomez - University of Toronto (source: attentions.pdf, page 0)\n","7. Łukasz Kaiser - Google Brain (source: attentions.pdf, page 0)\n","8. Illia Polosukhin - (source: attentions.pdf, page 0)\n","\n","Please note that the information provided is based on the given context.\n"]}],"source":[]},{"cell_type":"markdown","id":"09cfc58e-6995-4c1d-85b6-1cf185d480df","metadata":{},"source":["Try to ask the model something that is completely out of scope, and see what happens!"]},{"cell_type":"code","execution_count":null,"id":"7812e8d8-e650-4444-b616-36834abe6c23","metadata":{"executionCancelledAt":null,"executionTime":1558,"lastExecutedAt":1704819297846,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"What are the main problems to cook with olive oil?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["I'm sorry, but I don't have the information you're looking for. The provided information is from a PDF document titled \"codegen.pdf\" on page 5.\n"]}],"source":[]},{"cell_type":"markdown","id":"e9aeec8e-842f-4505-b895-8818cc7021fb","metadata":{},"source":["Try other queries and talk with your documents!"]},{"cell_type":"code","execution_count":null,"id":"9c4b26ec-dc92-49bc-9955-2516e8a0cca6","metadata":{"executionCancelledAt":1704727738652,"executionTime":49,"lastExecutedAt":1704708572393,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def asking_your_model(query, k):\n    # Define the chain\n    chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n    #Perform a similarity search. \n    matches = db_FAISS.similarity_search(query, k=k)\n    #We define both the text and the metadata obtain from the semantic search.\n    input_text = [x.page_content for x in matches]\n    input_metadata= [x.metadata for x in matches]\n    #We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n    meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in       the response\".format(input_metadata) \n    #We define an enriched query with the initial prompt and the metadata prompt. \n    enriched_query = query + meta_data_enriching\n    #We execute the chain. \n    response = chain.run(input_documents = matches, question = enriched_query)\n    return response\n    "},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"64a2c499-6ef6-424d-b58d-fbd571dd0522","metadata":{"executionCancelledAt":1704727738653,"executionTime":2198,"lastExecutedAt":1704708574592,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is functional correctness?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":97,"type":"stream"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e6eb01de-7644-45a0-a88c-f290ee351640","metadata":{"executionCancelledAt":1704727738654,"executionTime":3163,"lastExecutedAt":1704708577755,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is the multi-head attention in a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":217,"type":"stream"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0925bf69-fc5b-47d8-8f7d-322dab181ae3","metadata":{"executionCancelledAt":1704727738655,"executionTime":3217,"lastExecutedAt":1704708580972,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What are the main components of a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":137,"type":"stream"}}},"outputs":[],"source":[]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}
